\section{Introduction}
\vspace{-2.5mm}


Statistical tests based on distribution embeddings into reproducing kernel Hilbert spaces have been applied in many contexts,  including two sample testing \cite{HarBacMou08,gretton2012kernel,SugSuzItoKanetal11}, tests of independence \cite{gretton_kernel_2008,ZhaPetJanSch11,besserve_statistical_2013}, tests of conditional independence  \cite{fukumizu2007kernel,ZhaPetJanSch11}, and tests for higher order (Lancaster) interactions \cite{sejdinovic2013kernel}. %
% AG: if we are looking to save space in the bibliography we can cut the three references below. However it is important to have the references above, since they are more directly related to the theory-minded paper we're writing.
%Although relatively new, these tests have already influenced research beyond machine learning and proved to be useful tools in domains such as genomics \cite{Schweikert2013}, steganography \cite{Solanki2008} and econometrics \cite{zaremba2014measures}.       
For these tests,  consistency is guaranteed if and only if the observations are independent and identically distributed. 
Much real-world data fails to satisfy the i.i.d. assumption: audio signals, EEG recordings, text documents, financial time series, and samples obtained when running Markov Chain Monte Carlo, all show  significant temporal dependence patterns.  

The asymptotic behaviour of kernel test statistics becomes quite different when temporal dependencies exist within
the samples.
In recent work on independence testing using the Hilbert-Schmidt Independence
Criterion (HSIC) \cite{chwialkowski2014kernel}, the asymptotic distribution of the statistic under the null hypothesis is obtained for a pair of independent time series, which satisfy an absolute regularity or a $\phi$-mixing assumption.
In this case, the null distribution is shown to be an infinite weighted sum of {\em dependent} $\chi^2$-variables,
as opposed to the sum of \emph{independent} $\chi^2$-variables obtained in the i.i.d. setting \cite{gretton_kernel_2008}.
The difference in the asymptotic null distributions has important implications in practice:
under the i.i.d. assumption, an empirical estimate of the null distribution can be obtained by
repeatedly permuting the time indices of one of the signals. This breaks
the temporal dependence within the permuted signal, which causes the test to return an elevated
number of false positives, when used for testing time series. To address this problem, an alternative estimate of the null distribution
is proposed in \cite{chwialkowski2014kernel}, where the null distribution is simulated by repeatedly
{\em shifting} one
signal relative to the other. This preserves the temporal structure within each signal, while breaking the cross-signal
dependence.

A serious limitation of the shift procedure in \cite{chwialkowski2014kernel} is that it is specific
to the problem of independence testing: there is no obvious way to generalise it to  other
testing contexts. For instance, we might have two time series, with the goal of comparing
their marginal distributions - this is a generalization of the two-sample setting to which the shift
approach does not apply.

We note, however, that many kernel tests have a test statistic with a particular structure:  the Maximum Mean Discrepancy (MMD), HSIC, and the Lancaster interaction statistic,
each have empirical estimates which can be cast as normalized $V$-statistics,
$\frac{1} {n^{m-1}} \sum_{1\leq i_1,...,i_m \leq n} h(Z_{i_1},...,Z_{i_m})$,
where $Z_{i_1},...,Z_{i_m}$ are samples from a random process at the time points $\{i_1,\ldots,i_m\}$. We show that
a method of external randomization known as the {\em wild bootstrap} may be applied  \cite{leucht_dependent_2013,Shao2010} to simulate from the null distribution.
In brief, the arguments of the above sum are repeatedly multiplied by random, user-defined time series. For a test of level
$\alpha$, the $1-\alpha$ quantile of the empirical distribution obtained using these perturbed statistics serves as the test threshold. This approach has the important advantage over \cite{chwialkowski2014kernel} that it may be applied to {\em all} kernel-based tests for which $V$-statistics are employed, and not just for independence tests.

The main result of this paper is to show that the wild bootstrap procedure yields consistent tests for time series, i.e., tests based on the wild bootstrap  have a Type I error rate (of wrongly rejecting the null hypothesis) approaching the design parameter $\alpha$, and a Type II error (of wrongly accepting the null) approaching zero, as the number of samples increases. We use this result to construct a two-sample test using MMD, and an independence test using HSIC. The latter procedure is applied both to testing for instantaneous independence, and to testing for independence across multiple time lags, for which the earlier shift procedure of \cite{chwialkowski2014kernel} cannot be applied.

%DS: commented as we have the same point in the next paragraph...Out test against long range dependence is demonstrated to have an improved power in comparison to the related test by \cite{besserve_statistical_2013}.

%% AG: I moved the detailed MCMC discussion to the MCMC section

%Further, we construct a test of time series dependence similar to one proposed by \cite{besserve_statistical_2013} showing improvements in terms of detection errors.            

%Simultaneous testing for dependence of two time series across multiple time lags is also of interest, since it may not be a priori known at what lag the dependence occurs \cite{besserve_statistical_2013}. In this case, the shift procedure of \cite{chwialkowski2014kernel} can be problematic, as certain shifts may increase dependence between the time series, and it cannot be known in advance which shifts these will be. By contrast, the wild bootstrap can be used straightforwardly in this circumstance, and outperforms \cite{besserve_statistical_2013} in experiments.

We begin our presentation in Section \ref{sec:background}, with a review of the $\tau$-mixing assumption required of the time series, as well as of   $V$-statistics (of which MMD and HSIC are instances). We also introduce the form taken by the wild bootstrap. In Section \ref{sec:main}, we establish a general consistency result for the wild bootstrap procedure on $V$-statistics, which we apply to MMD and to HSIC in Section \ref{sec:mmd_hsic}. Finally, in Section \ref{sec:Experiments}, we present a number of empirical comparisons: in the two sample case, we test for differences in audio signals with the same underlying pitch, and present a performance diagnostic for the output of a Gibbs sampler (the MCMC M.D.); in the independence case, we test for independence of two time series sharing a common variance (a characteristic of econometric models), and compare against the test of \cite{besserve_statistical_2013} in the case where dependence may occur at multiple, potentially unknown lags. Our tests outperform both the naive approach which neglects the dependence structure within the samples, and the approach of \cite{besserve_statistical_2013}, when testing across multiple lags. Detailed proofs are found in the appendices of an accompanying technical report \cite{chwialkowski2014wild}, which we reference from the present document as needed.

%\vspace{-2mm}
\section{Background}\label{sec:background}
%\vspace{-2mm}
The main results of the paper are based around two concepts: $\tau$-mixing \cite{dedecker2007weak}, which describes the dependence within the time series, and  $V$-statistics \cite{serfling80}, which constitute our test statistics. In this section, we review these topics, and introduce the concept of wild bootstrapped $V$-statistics, which will be the key ingredient in our test construction.
%\vspace{-4mm}
\paragraph{$\tau$-mixing.} The notion of $\tau$-mixing is used to characterise weak dependence. It is a less restrictive alternative to classical mixing coefficients, and is covered in depth in \cite{dedecker2007weak}. Let $\{Z_t,\mathcal{F}_t\}_{t \in \mathbb{N}}$  be a stationary sequence of integrable random variables, defined on a probability space $\Omega$ with a probability measure $P$ and a natural filtration $\mathcal{F}_t$. The process  is called $\tau$-dependent if 
\begin{align*}
\tau(r) &= \sup_{l \in \mathbb{N}} \frac 1 l \sup_{ r \leq i_1 \leq ... \leq i_l} \tau( \mathcal F_0,(Z_{i_1},...,Z_{i_l}) )  \overset{r \to \infty}{\longrightarrow} 0,\;\text{where} \\
\tau(\mathcal{M},X) &=  \ev \left( \sup_{g \in \Lambda} \left| \int g(t) P_{X|\mathcal{M}}(dt) - \int g(t) P_X(dt) \right| \right)
\end{align*}
and $\Lambda$ is the set of all one-Lipschitz continuous real-valued functions on the domain of $X$. $\tau(\mathcal M,X)$ can be interpreted as the minimal $L_1$ distance between $X$ and $X^*$ such that $X \overset{d}{=}X^*$ and $X^*$ is independent of $\mathcal M \subset \mathcal F$. Furthermore, if $\mathcal F$ is rich enough, this $X^*$ can be constructed (see Proposition \ref{prop:Coupling} in the Appendix). More information is provided in the Appendix \ref{append:differentMixing}.

%  Note that this mixing definition differs from commonly used notions of mixing, such as $\alpha, \beta$, or $\phi$ mixing, some of which were required in the previous work \cite{chwialkowski2014kernel}. We describe in more detail how these notions of dependence are related in Appendix \ref{append:differentMixing}.
%  
%\vspace{-4mm}
\paragraph{$V$-statistics.} The test statistics considered in this paper are always $V$-statistics. Given the observations $Z=\left\{Z_t\right\}_{t=1}^n$, a $V$-statistic of a symmetric function $h$ taking $m$ arguments is given by 
\begin{equation}
\label{def:Vstat}
V(h,Z) = \frac{1}{n^m} \sum_{i \in N^m} \nolimits h(Z_{i_1},...,Z_{i_m}),
\end{equation}
where $N^m$ is a Cartesian power of a set $N= \{1,...,n\}$. For simplicity, we will often drop the second argument and write simply $V(h)$. 
%DS: commented as it does not appear anywhere in the main text...We will  denote the tuple $(i_1,...,i_m)$ by $i$.
%i.e. $\sum_{i \in N^m} f(\cdot) \equiv \sum_{(i_1,...,i_m) \in N^m} f()$. 

We will refer to the function $h$ as to the \emph{core} of the $V$-statistic $V(h)$. While such functions are usually called kernels in the literature, in this paper we reserve the term kernel for positive-definite functions taking two arguments. A core $h$ is said to be $j$-degenerate if for each $z_1,\ldots,z_j$ $\ev h(z_1,\ldots , z_j , Z_{j+1}^*,\ldots ,Z_m^*) = 0,$ where $Z_{j+1}^*,\ldots,Z_m^*$ are independent copies of $Z_1$. If $h$ is $j$-degenerate for all $j\leq m-1$, we will say that it is \emph{canonical}. For a one-degenerate core $h$, we define an auxiliary function $h_2$, called the second component of the core, and given by $h_2(z_1,z_2) = \ev h(z_1,z_2, Z_3^*,\ldots, Z_m^*).$ Finally we say that $nV(h)$ is a normalized $V$-statistic, and that a $V$-statistic with a one-degenerate core is a degenerate $V$-statistic.  This degeneracy is common to many kernel statistics when the null hypothesis holds \cite{gretton2012kernel,gretton_kernel_2008,sejdinovic2013kernel}.

Our main results will rely on the fact that $h_2$ governs the asymptotic behaviour of normalized degenerate $V$-statistics. Unfortunately, the limiting distribution of such $V$-statistics is quite complicated - it is an infinite sum of \emph{dependent} $\chi^2$-distributed random variables, with a dependence  determined by the temporal dependence structure within the process $\{Z_t\}$ and by the eigenfunctions of a certain integral operator associated with $h_2$ \cite{i._s._borisov_orthogonal_2009,chwialkowski2014kernel}. Therefore, we propose a bootstrapped version of the $V$-statistics which will allow a consistent approximation of this difficult limiting distribution.  

%\vspace{-4mm}
\paragraph{Bootstrapped $V$-statistic.} 
We will study two versions of the bootstrapped $V$-statistics  
\begin{align}
 V_{b1}(h,Z) = \frac{1}{n^m} \sum_{i \in N^m} \nolimits W_{i_1,n} W_{i_2,n} h(Z_{i_1},...,Z_{i_m}), \label{Vb1}\\ 
 V_{b2}(h,Z) = \frac{1}{n^m} \sum_{i \in N^m}  \nolimits \tilde W_{i_1,n}  \tilde W_{i_2,n} h(Z_{i_1},...,Z_{i_m}),\label{Vb2}
\end{align}
where $\{W_{t,n}\}_{1 \leq t \leq n }$ is an auxiliary wild bootstrap process and $\tilde W_{t,n} = W_{t,n} - \frac 1 n \sum_{j=1}^n W_{j,n}$. This auxiliary process, proposed by \cite{Shao2010,leucht_dependent_2013}, satisfies the following assumption:

\emph{Bootstrap assumption:} $\{W_{t,n}\}_{1 \leq t \leq n }$ is a row-wise strictly stationary triangular array independent of all $Z_t$ such that $\ev W_{t,n}=0$ and $\sup_{n} \ev|W_{t,n}^{2+\sigma}| < \infty$ for some $\sigma > 0$. The autocovariance of the process is given by $\ev W_{s,n} W_{t,n}=\rho(|s-t|/l_n)$ for some function $\rho$, such that $\lim_{u \to 0} \rho(u) = 1$ and $\sum_{r=1}^{n-1} \rho(|r|/l_n)= O(l_n)$. The sequence $\left\{l_n\right\}$ is taken such that $l_n=o(n)$ but $\lim_{n \to \infty} l_n = \infty$. The variables $W_{t,n}$  are $\tau$-weakly dependent with coefficients $\tau(r) \leq C \zeta^{\frac{r} {l_n}}$ for $r=1,...,n$, $\zeta \in (0,1)$ and $C\in\mathbb R$.

As noted in in \cite[Remark 2]{leucht_dependent_2013}, a simple realization of a process that satisfies this assumption is $W_{t,n} = e^{-1/l_n}W_{t-1,n} + \sqrt{1 -e^{-2/l_n}} \epsilon_t$
where $W_{0,n}$ and $\epsilon_1,\ldots,\epsilon_n$ are independent standard normal random variables. For simplicity, we will drop the index $n$ and write $W_t$ instead of $W_{t,n}$. A process that fulfils the \emph{bootstrap assumption} will be called  \emph{bootstrap process}. Further discussion of the wild bootstrap is provided in the Appendix  \ref{wildintro}.
% AG: removed paragraph break to save space
The versions of the bootstrapped $V$-statistics in \eqref{Vb1} and \eqref{Vb2} were previously studied in \cite{leucht_dependent_2013} for the case of canonical cores of degree $m=2$. We extend their results to higher degree cores (common within the kernel testing framework), which are not necessarily one-degenerate. When stating a fact that applies to both $V_{b1}$ and $V_{b2}$, we will simply write $V_b$, and the argument $Z$ will be dropped when there is no ambiguity. 
%\vspace{-2.5mm}
\section{Asymptotics of wild bootstrapped $V$-statistics}\label{sec:main}
%\vspace{-2.5mm}
In this section, we present main Theorems that describe asymptotic behaviour of $V$-statistics. In the next section, these results will be used to construct kernel-based statistical tests applicable to dependent observations. Tests are constructed so that the  $V$-statistic is degenerate under the null hypothesis and non-degenerate under the alternative. Theorem \ref{th:mainOne} guarantees that the bootstrapped $V$-statistic will converge to the same limiting null distribution as the simple $V$-statistic. 

% Following \cite{leucht_dependent_2013}, we will establish the convergence of the bootstrapped distribution to the desired asymptotic distribution in the Prokhorov metric $\varphi$ \cite[Section 11.3]{dudley2002real}), and ensure that this distance approaches zero \emph{in probability} as $n\rightarrow\infty$. This two-part convergence statement is needed due to  the additional randomness introduced by the $W_{j,n}$.

%since distributions of the bootstrapped statistics are random, 

%AG: I modified and shortened the wording.

%This notion can be  expressed  in terms of convergence in Prokhorov metric $\varphi$ \cite[Section 11.3]{dudley2002real}. Indeed by \cite[Theorem 11.3.3]{dudley2002real}, since values of $V$-statistics are real numbers, convergence in distribution is equivalent to the convergence in Prokhorov metric. 

Throughout this paper we will make one mild assumption
\[
  \sup_{i \in N^m}\ev h(Z_i)^2 < \infty,
 \]
 where $Z_i = (Z_{i_1},\cdots Z_{i_m})$. This assumption is almost always automatically satisfied, since most of the kernels used in practice are bounded.
\begin{Theorem}
\label{th:mainOne}
Assume that the stationary process $Z_t$ is $\tau$-dependent with $\sum_{r=1}^\infty r^2 \sqrt{\tau(r)} < \infty$. If the core $h$ is a Lipschitz continuous, one-degenerate and its $h_2$-component is a positive definite kernel, such that $\ev h_2(Z_0,Z_0) < \infty$, then $nB_n$ \eqref{Vb1}, \eqref{Vb2},  and $n V_n$  \eqref{def:Vstat} converge weakly to the same distribution $V$.  Moreover $nB_n(h_2)$ and $nV_n(h_2)$ converge weakly to $\binom {m} {2} ^{-1} V$.
\end{Theorem}
\begin{proof}
In the proof we are going to use \cite{leucht_dependent_2013}[Theorems 2.1, 3.1], which characterise asymptotic properties of $nV_n(h_2)$ and $n B(h_2)$. Both  theorems use similar set of assumptions which we verify upfront.  \\
\textit{Assumption A2.}\begin{itemize}
 \item (i)  $h_2$ is one-degenerate and symmetric - this follows from the Hoeffding decomposition;
 \item (ii) $h_2$ is a kernel - is one of the assumptions of this theorem;
 \item (iii) $\ev h_2(Z_1,Z_1) < \infty$ -- follows from $\sup_{i \in N^6}|\ev h(Z_i) |<\infty$ ;
 \item  (iv) $h_2$ is Lipschitz continuous - follows from the Lemma \ref{stm:LipAndBound}.
\end{itemize}
\textit{Assumption B1, A1.} Assumption $B1$, $\sum_{r=1}^n r^2 \sqrt{\tau(r)} < \infty$, is the same as ours, assumption $A1$, $\sum_{r=1}^n  \sqrt{\tau(r)} < \infty$ is implied.\\
\textit{Assumption B2.} This assumption about the bootstrap process $W_t$ is the same as our Definition \ref{bootstrapAss}. 


Denote by $V$ the weak limit of $n V_n(h_2)$, which exits by the  \cite{leucht_dependent_2013}[Theorem 2.1],  and let $\mathcal{F} = \sigma(Z_1, \cdots , Z_n)$.   By \cite[Theorem 3.1]{leucht_dependent_2013}, since the distribution of  $V$ is continuous, we have 
\begin{align*}
\sup_{x \in R} &\left| P(nB_n(h_2)  < x|\mathcal{F}) -   P(V<x) \right| \to 0  \\
\end{align*}
in probability. We show that $nB_n(h_2)$ converges  to $V$ weakly, by showing  pointwise convergence  of CDF  
\begin{align*}
 \lim_{n \to \infty} &P(nB_n(h_2)  < x) =  \lim_{n \to \infty} \ev P(nB_n(h_2)  < x|\mathcal{F}) \\
 &=  \ev  \lim_{n \to \infty} P(nB_n(h_2)  < x|\mathcal{F})  = \ev P(V<x) =P(V<x) 
\end{align*}
To change the order of limit and expectation we have dominated convergence Theorem, justified since  $P(nB_n(h)  < x|\mathcal{F})$  are bounded by 1.
%TODO mybe write more
The difference $n(B_n(h) - V_n(h))$ is
\begin{align*}
 n \left (B_n(h) -  \binom m 2 B_n(h_2) \right) + \binom m 2 \left (n B_n(h_2) -V\right)+ \left (\binom m 2 V - nV_n(h)\right)
 \end{align*}
By  Lemma \ref{lem:equivBoot} and Lemma \ref{lem:equivVanila} respectively, both 
$$n (B(h) -   \binom m 2  B(h_2)) , n (V_n(h) - n \binom m 2 V_n(h_2))$$
converge to zero in mean square. We check assumptions: since $Z_t$ is tau mixing and $h$ is Lipschitz continuous, by Lemma \ref{lem:disentangle} all self products of components and $Z_t$, $(h_c \times h_c,Z_t)$ for $c>0$, are $\varDelta$ type of order $\tau(r)$, of order at least  $o(r^{-4})$ (since $\sum_{r=1}^n r^2 \sqrt{\tau(r)} < \infty$). Since $h$ is one degenerate,  first and zero component $h_0,h_1$ are equal to zero (and so are $B(h_0),B(h_1)$).  

This shows that $nB_n(h_2)$ converges weakly to $V$. 
\end{proof}  

On the other hand, if the $V$-statistic is not degenerate, which is usually true under the alternative, it converges to some non-zero constant. 
\begin{Theorem}
\label{th:mainThree}
Assume that the stationary process $Z_t$ is $\tau$-dependent with $\tau(r) = o(r^{-4})$. If the core $h$ is a Lipschitz continuous, and  $h_0$ component is positive then  $V_n$ converges in mean squared to $h_0$.
\end{Theorem}
In this setting, Theorem \ref{th:mainTwo} guarantees that the bootstrapped $V$-statistic will converge to zero in probability. This property is necessary in testing, as it implies that the test thresholds computed using the bootstrapped $V$-statistics will also converge to zero, and so will the corresponding Type II error.   
\begin{Theorem}
\label{th:mainTwo}
Assume that the stationary process $\{Z_t\}$ is $\tau$-dependent with a coefficient $\tau(r) = o(r^{-4})$. If the core $h$ is  a function of $m>1$ arguments then $B_1(h)$ and $o(n) B_2(h)$  converge to zero in mean squared. 
\end{Theorem}
Although both $B_2$ and $B_1$  converge to zero, the rate does not seem to be that same. As a consequence, tests that utilize $B_2$ usually give lower Type II error then the ones that use $B_1$. On the other hand, $B_1$ seems to better approximate $V$-statistic distribution under the null hypothesis. This agrees with our experiments in Section \ref{sec:Experiments1} as well as with those in \cite[Section 5]{leucht_dependent_2013}).  
These results a sufficient for adopting kernel tests developed for i.i.d. data to tests that work on random processes. In particular Theorem \ref{th:mainOne}  justifies usage of bootstraped $V$-statistics for estimating quantiles of the null distribution, while Theorems \ref{th:mainThree}\ref{th:mainTwo} guarantee consistency.

The general testing procedure is 

\begin{itemize}
\item Calculate the test statistic $n V_{n}(h)$.

\item Obtain wild bootstrap samples $\{B_{n}(h)\}_{i=1}^{D}$
and estimate the $1-\alpha$ empirical quantile of these samples. 
\item If $n V_{n}(h)$ exceeds the quantile, reject.
\end{itemize}




\section{Applications to Kernel Tests}\label{sec:mmd_hsic}
%\vspace{-2.5mm}
In this section, we describe how the wild bootstrap for $V$-statistics can be used to construct kernel tests for independence and the two-sample problem, which are applicable to weakly dependent observations. We start by reviewing the main concepts underpinning the kernel testing framework.

For every symmetric, positive definite function, i.e., \emph{kernel} $k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$,
there is an associated reproducing kernel Hilbert space $\mathcal{H}_{k}$ \cite[p. 19]{BerTho04}.  The kernel embedding of a probability measure
$P$ on $\mathcal{X}$ is an element $\mu_{k}(P)\in\mathcal{H}_{k}$,
given by $\mu_{k}(P)=\int k(\cdot,x)\, dP(x)$ \cite{BerTho04,SmoGreSonSch07}.
If a measurable kernel $k$ is bounded, the mean embedding $\mu_{k}(P)$
exists for all probability measures on $\mathcal{X}$, and for many interesting
bounded kernels $k$, including the Gaussian, Laplacian and inverse
multi-quadratics, the kernel embedding $P\mapsto\mu_{k}(P)$ is injective.
Such kernels are said to be \emph{characteristic} \cite{SriGreFukLanetal10}.
The RKHS-distance $\left\Vert \mu_k(P_x)-\mu_k(P_y)\right\Vert_{{\mathcal H}_k}^2$ between embeddings of two probability measures $P_x$ and $P_y$
is termed the Maximum Mean Discrepancy (MMD), and its empirical version serves as a popular statistic for non-parametric two-sample testing \cite{gretton2012kernel}.
Similarly, given a sample of paired observations $\{(X_i,Y_i)\}_{i=1}^n\sim P_{xy}$, and kernels $k$ and $l$ respectively on $X$ and $Y$ domains, the RKHS-distance 
$\left\Vert \mu_\kappa(P_{xy})-\mu_\kappa(P_x P_y)\right\Vert_{{\mathcal H}_{\kappa}}^2$ between embeddings of the joint distribution and of the product of the marginals, measures dependence between $X$ and $Y$. Here, $\kappa((x,y),(x',y'))=k(x,x')l(y,y')$ is the kernel on the product space of $X$ and $Y$ domains.
This quantity is called Hilbert-Schmidt Independence Criterion (HSIC) \cite{gretton_measuring_2005,gretton_kernel_2008}. When characteristic RKHSs are used, the HSIC is zero iff $X \indep Y$: this follows from \cite{Asimplercondition}.
The  empirical statistic is written $\widehat{\text{HSIC}}_{\kappa} = \frac{1}{n^2}\text{Tr}(KHLH)$ for kernel matrices $K$ and $L$ and the centering matrix $H=I-\frac{1}{n}\mathbf{1}\mathbf{1}^\top$.

In this section, we describe how the wild bootstrap for $V$-statistics can be used to construct kernel tests for independence and the two-sample problem, in presence of weakly dependent observations. The main concepts underpinning the kernel testing framework are reviewed in the section \ref{sec:FBackground}.

%\vspace{-2mm}
\subsection{Wild Bootstrap For MMD}
%\vspace{-2mm}
Denote the observations by $\{X_i\}_{i=1}^{n_x}\sim P_x$, and $\{Y_j\}_{j=1}^{n_y}\sim P_y$. Our goal is to test the null hypothesis $\mathbf H_0: P_x=P_y$ vs. 
the alternative $\mathbf H_1: P_x\neq P_y$. In the case where samples have equal sizes, i.e., $n_x=n_y$, application of the wild bootstrap to MMD-based tests on dependent samples is straightforward: the empirical MMD can be written as a $V$-statistic with the core of degree two on pairs $z_i=(x_i,y_i)$ given by $h(z_1,z_2) = k(x_1,x_2)- k(x_1,y_2) - k(x_2,y_1) + k(y_1,y_2)$. It is clear that whenever $k$ is Lipschitz continuous and bounded, so is $h$. Moreover, $h$ is a valid positive definite kernel, since it can be represented as an RKHS inner product  $\left\langle k(\cdot, x_1) -k(\cdot, y_1),k(\cdot, x_2) -k(\cdot, y_2) \right\rangle_{\Hk}$. Under the null hypothesis, $h$ is also one-degenerate, i.e., $\ev h\left((x_1,y_1),(X_2,Y_2)\right) = 0$. Therefore, we can use the bootstrapped statistics in \eqref{Vb1} and \eqref{Vb2} to approximate the null distribution and attain a desired test level.

When $n_x\neq n_y$, however, it is no longer possible to write the empirical MMD
%\begin{align*}
%\widehat{\text{MMD}}_k=\frac{1}{n_x^2}\sum_{i=1}^{n_x}\sum_{j=1}^{n_x}k(x_i,x_j)-\frac{1}{n_x^2}\sum_{i=1}^{n_y}\sum_{j=1}^{n_y}k(y_i,y_j)-\frac{2}{n_x n_y}\sum_{i=1}^{n_x}\sum_{j=1}^{n_y}k(x_i,y_j) 
%\end{align*}
as a one-sample $V$-statistic. We will therefore require the following bootstrapped version of MMD
\begin{align}
\widehat{\text{MMD}}_{k,b}&=\frac{1}{n_x^2}\sum_{i=1}^{n_x}\sum_{j=1}^{n_x}\tilde W_i^{(x)}\tilde W_j^{(x)}k(x_i,x_j)-\frac{1}{n_x^2}\sum_{i=1}^{n_y}\sum_{j=1}^{n_y}\tilde W_i^{(y)}\tilde W_j^{(y)}k(y_i,y_j)\notag\\
{}&\qquad-\frac{2}{n_x n_y}\sum_{i=1}^{n_x}\sum_{j=1}^{n_y}\tilde W_i^{(x)}\tilde W_j^{(y)}k(x_i,y_j),\label{eq:mmdkb}
\end{align}
where $\tilde W_t^{(x)}=W_t^{(x)}-\frac{1}{n_x}\sum_{i=1}^{n_x}W_i^{(x)}$, $\tilde W_t^{(y)}=W_t^{(y)}-\frac{1}{n_y}\sum_{j=1}^{n_y}W_j^{(y)}$;  $\{W_t^{(x)}\}$ and $\{W_t^{(y)}\}$ are two auxiliary wild bootstrap processes that are independent of $\left\{ X_t \right\}$ and $\left\{ Y_t \right\}$ and also independent of each other, both satisfying the bootstrap assumption in Section \ref{sec:background}.  
The following Proposition shows that the bootstrapped statistic has the same asymptotic null distribution as the empirical MMD. The proof follows that of \cite[Theorem 3.1]{leucht_dependent_2013}, and is given in the Appendix.

\begin{proposition}\label{prop:mmd}
 Let $k$ be bounded and Lipschitz continuous, and let $\left\{ X_t \right\}$ and $\left\{ Y_t \right\}$ 
 both be $\tau$-dependent with coefficients $\tau(r) =  O(r^{-6-\epsilon})$, but independent of each other. Further, let $n_x=\rho_x n$ and $n_y=\rho_y n$ where $n=n_x+n_y$. Then, under the null hypothesis $P_x=P_y$, $\varphi\left(\rho_x \rho_y n\widehat{\text{MMD}}_k, \rho_x \rho_y n\widehat{\text{MMD}}_{k,b}\right)\to 0$ in probability as $n\to\infty$, where $\varphi$ is the Prokhorov metric and $\widehat{\text{MMD}}_k$ is the MMD between empirical measures.
\end{proposition}

\subsection{Wild Bootstrap For HSIC}\label{sec:hsic}

Recall (\ref{eq:hsicOMG}) that the core of the test static for HSIC, with notation $z_i = (x_i,y_i)$, is  
\begin{equation*}
\begin{split}
h(&z_1,z_2,z_3,z_4) = \frac{1}{4!} \sum_{\pi \in S_4}  k(x_{\pi(1)},x_{\pi(2)}) [  l(y_{\pi(1)},y_{\pi(2)})  \\
 &+  l(y_{\pi(3)},y_{\pi(4)}) - 2  l(y_{\pi(2)},y_{\pi(3)})] . \\
\end{split}  
\end{equation*}
One-degeneracy of the core under the null hypothesis was stated in \cite[Theorem 2]{gretton_kernel_2008}, \cite[Section A.2, following eq. (11)]{gretton_kernel_2008} shows that $h_2$ is a kernel; $h_0\geq 0$ follows from the fact that HSIC is a distance. Using Theorems \ref{th:mainOne},\ref{th:mainTwo},\ref{th:mainThree} we can construct an independence test using $h$. Drawback of this test, when implemented in the most straightforward way,  is its quadruple computational complexity. To achieve quadratic time complexity, that matches time complexity of HSIC test for i.i.d. data, we modify our bootstrapped statistic.

\paragraph{Quadratic time HSIC.}
In this section we assume that kernels $k,l$ are positive and bounded. We define empirical mean embedding $\tilde \mu_X(x) = \frac 1 n \sum_{i}^n k(x,X_i) $ and centred kernels
\begin{equation*}
\begin{split}
\bar{k}(x,x') =& k(x',x) - \ev k(x,X) -\ev k(X',x') + \ev k(X,X')\\
=&\langle k(x,\cdot ) -\mu_X ,k(x',\cdot)- \mu_X\rangle. \\
 \tilde k(x,x') =& k(x,x') - \frac 1 n \sum_{i}^n k(x,X_i) - \frac 1 n \sum_{i}^n k(x',X_i) +   \frac {1} {n^2} \sum_{i,j}^n k(X_j,X_i)\\
=&\langle k(x,\cdot ) - \tilde\mu_X ,k(x',\cdot)- \tilde \mu_X\rangle. \\
\end{split}
\end{equation*}
where $X,X'$ are i.i.d. copies of $X_1$. Same definitions hold for the kernel $l$. Let $Q_i$ denote  $W_i$ or  $\tilde W_i$ (where  it is necessary, we check claims for both $W_i$ and  $\tilde W_i$ separately). We further define 
\begin{align}
\label{hsic_2}
S_n &= \frac {1} {\sqrt n}\sum_{i \in N} Q_i ( \phi(X_i) - \tilde \mu_X ) \otimes (\phi(Y_i )  - \tilde \mu_Y), \\
T_n &= \frac {1} {\sqrt n}  \sum_{i \in N} Q_i ( \phi(X_i) -  \mu_X ) \otimes(\phi(Y_i )  -  \mu_Y). 
\end{align}
First, we  relate $T_n$ to $B(h_2)$. 
\begin{statement}{ \cite[section A.2, following eq. (11)]{gretton_kernel_2008}}
\label{stm:thanks}
The second component of $h$ is $h_2(z_1,z_2) =  \frac 1 6 \bar{k}(x_1,x_2) \bar{l}(y_1,y_2).$ 
\end{statement}
\begin{lemma}
\label{lem:T_n}
 Squared norm of $T_n$  is equal to $ 6 B(h_2) $.
\end{lemma}
\begin{proof}
  \begin{align*}
 \| T_n \|^2 =  & \frac 1 n  \sum_{i,j \in N} Q_i Q_j \bigg\langle   ( \phi(X_i) -  \mu_X ) \otimes  (\phi(Y_i )  -  \mu_Y),   ( \phi(X_j) -  \mu_X ) \otimes (\phi(Y_j )  -  \mu_Y) \bigg\rangle  \\
  =& \frac 1 n \sum_{i,j \in N} Q_i Q_j \bar k(X_i,X_j)  \bar l(Y_i,Y_j)  \\
  =&6 B(h_2).
 \end{align*}
\end{proof}
Next we relate $S_n$ to $T_n$ -- we  show that the difference between them is asymptotically negligible. We start with a technical lemma.  
\begin{lemma}
\label{lem:hahaha}
If $(\bar k \times \bar k,Z_i)$  is of type $\varDelta$   of order $O(r^{-4})$ (see Definition \ref{def:varDelta}), then 
$$\lim_{n \to \infty} \ev \left \| \sqrt n (\tilde  \mu_X  - \mu_X) \right \|^4 = O(1).$$
\end{lemma}
\begin{proof}
 \begin{align*}
   \ev \left \| \sqrt n (\tilde  \mu_X  -  \mu_X) \right \|^4  &=  \ev \left \|  \frac {1} {\sqrt n}  \sum_{i \in N} \phi(X_i) - \mu_X  \right\|^4  \\
     &=\ev  \left( \frac 1 n   \sum_{i \in N} \langle  \phi(X_j) - \mu_X, \phi(X_i) - \mu_X \rangle  \right)^2 \\
   &=\frac{1}{n^2} \ev  \sum_{i \in N^4} \bar k \times \bar k (Z_i). 
 \end{align*}
 Since $(\bar k \times \bar k, X_i)$  is of type $\varDelta$, by Lemma \ref{lem:higherVstats}, the expected value is of order $O(1)$.
\end{proof}

\begin{lemma}
\label{lem:difS_nT_n}
 If $(\bar k \times \bar k,Z_i)$, $(\bar l \times \bar l,Z_i)$  are of type $\varDelta$ of order $O(r^{-4})$, then, under the null,  $\|S_n\|^2- \|T_n\|^2$  converges to zero in mean square. Under the alternative $\frac 1 n (\|S_n\|^2- \|T_n\|^2) $ converges to zero in  mean square.
\end{lemma}
\begin{proof}
%wild bootstrap validate ?
% p.44 ?
We first show that $\ev \| S_n- T_n\|^2 \to 0$ both under the null and the alternative. Then,  using the fact that $\|T_n\|^2< \infty$ under the null  and $\frac 1 n \|T_n\|^2< \infty $ under alternative we  will conclude the proof. The difference $S_n- T_n$ is 
\begin{align*}
\label{eq:diferencesAgain}
 \frac {1} { \sqrt n} &\sum_{i \in N}Q_i \bigg[ (\phi(X_i) - \tilde \mu_X ) \otimes (\phi(Y_i )  - \tilde \mu_Y) - ( \phi(X_i) -  \mu_X ) \otimes(\phi(Y_i )  -  \mu_Y)\bigg] \\
&= \frac {1} { \sqrt n} \sum_{i \in N} Q_i \bigg[    \phi(X_i)  \otimes   \mu_Y -\phi(X_i) \otimes \tilde \mu_Y \bigg]  \\
&+\frac {1} { \sqrt n} \sum_{i \in N} Q_i \bigg[    \phi(Y_i)  \otimes   \mu_X -\phi(Y_i) \otimes \tilde \mu_X \bigg] \\
&+\frac {1} { \sqrt n} \sum_{i \in N}Q_i  ( \tilde  \mu_X   \otimes \tilde \mu_Y - \mu_Y \otimes \mu_X).
\end{align*}
We  examine differences separately -- it is sufficient to show that each difference converges to zero in mean square. 

The expected  norm of the first difference  is
\begin{align*}
  \ev &\bigg \|\frac {1} { \sqrt n}\sum_{i \in N} Q_i \bigg[   \phi(X_i)  \otimes   \mu_Y -\phi(X_i) \otimes \tilde \mu_Y \bigg] \bigg \|^2\\
  &=\ev \bigg \| \sqrt n(  \mu_Y-  \tilde  \mu_Y) \otimes \frac {1} { \sqrt n} \sum_{i \in N} Q_i  \phi(X_i)  \bigg \|^2  \\
 & \leq \sqrt {\ev \bigg \| \sqrt n( \tilde \mu_Y - \mu_Y ) \bigg \|^4 \ev \bigg \| \frac {1} { \sqrt n} \sum_{i \in N} Q_i  \phi(X_i)  \bigg \|^4}.
 \end{align*}
We used $\|v \otimes u\| = \|v \|\| u\|$ and Cauchy-Schwarz inequality.  By Lemma \ref{lem:hahaha} the first term is $O(1)$. The second term is equal to 
 \begin{align*}
 \ev  \|  \frac {1} { n} \sum_{i \in N} Q_i  \phi(X_i)    \|^4 = \ev  \left(\frac {1} { n^2}  \sum_{i,j} k(X_i,X_j)Q_iQ_j \right)^2.
 \end{align*}
The expected value converges  to zero in mean square by  Lemma \ref{lem:higherVstats} (the assumption $\sup_{i,j}  k(X_i,X_j) < \infty$ is satisfied). Using similar reasoning, the second term 
\begin{align*}
 \ev &\bigg \|\frac {1} { \sqrt n}\sum_{i \in N} Q_i \bigg[  \phi(Y_i) \otimes \tilde \mu_X  -\phi(Y_i)  \otimes   \mu_X \bigg] \bigg \|^2
\end{align*}
 also converges to zero. The final term is 
 \begin{align*} 
 \ev  &\bigg \|  \frac {1} { \sqrt n} \sum_{i \in N}Q_i  ( \tilde  \mu_X   \otimes \tilde \mu_Y - \mu_Y \otimes \mu_X)\bigg \|^2  \\
 &=\ev \left| \frac {1} {  n} \sum_{i \in N}Q_i \right | \ev \bigg \|  \sqrt n (\tilde  \mu_X   \otimes \tilde \mu_Y - \mu_Y \otimes \mu_X )\bigg \|^2 \\
\end{align*}
$ \frac {1} { n} \sum_{i \in N}Q_i$ converges in  mean square to zero (Lemmas \ref{lem:meanWi}, \ref{stmt:obviousD}). We rewrite the second term  
\begin{align*}
 \ev \bigg \| \sqrt n (\tilde  \mu_X   \otimes \tilde \mu_Y -  \tilde\mu_Y \otimes \mu_X + \tilde \mu_Y \otimes \mu_X - \mu_Y \otimes \mu_X) \bigg \|^2 
 \end{align*}
It is sufficient to bound  
 \begin{align*}
 \ev \bigg \|  \sqrt n \tilde \mu_Y \otimes ( \tilde \mu_X   -\mu_X) \bigg \|^2 & \leq  \ev  \sqrt {\bigg \|  \tilde \mu_Y \bigg \|^4 \ev \bigg \| \sqrt n ( \tilde \mu_X   -\mu_X) \bigg \|^4}\\
   \ev \bigg \|  \sqrt n \mu_X \otimes ( \tilde \mu_Y   -\mu_Y) \bigg \|^2  &=    \bigg \|  \mu_X \bigg \|^2  \ev \bigg \|\sqrt n ( \tilde \mu_Y   -\mu_Y) \bigg \|^2  \\
\end{align*}
$ \ev   \| \tilde \mu_Y \|^4 = E \frac {1} {n^4} \sum_{i \in N^4} (l \times l)(Y_i) = O(1)$, since $ l$ is bounded. By Lemma \ref{lem:hahaha} $\ev \big \| \sqrt n ( \tilde \mu_X   -\mu_X) \big \|^4$ and $ \ev \big \|\sqrt n ( \tilde \mu_Y   -\mu_Y) \big \|^2$ are finite. Thus, to whole expression converges to zero. We proved  that $T_n-S_n$ converges in  mean square to zero.
We have 
%why not absolute value
\begin{align*}
  \ev | \|T_n\|^2 - \|S_n\|^2 |  & \leq\ev  \big | \|T_n\| - \|S_n\| \big | \big |  \|T_n\| +  \|S_n\| \big | \\
  &\leq \sqrt{ \ev \big | \|T_n\| - \|S_n\| \big |^2  \ev \big| \|T_n\| +  \|S_n\| \big|^2 }\\ 
\end{align*}
To show that the above expression converges to zero it is sufficient to show that  $\ev \|T_n\|^2< \infty$ and  $\ev  \|S_n\|^2< \infty$.
Under the null hypothesis, by Lemma \ref{lem:higherVstats}, expected value of   $\ev \|T_n\|^2 =n B_n(h_2) $ is finite. 
Since $\ev \| T_n -S_n\|^2 \to 0$ we also have  $\ev \| T_n -S_n\| \to 0$. $ \ev \|S_n\|$ is therefore finite since
\[
 \ev \|S_n\| = \ev \|S_n - T_n +T_n\| \leq \ev \|S_n - T_n\| + \ev \|T_n\|< \infty 
\]
Therefore we have 
%parallegram identity how?
\begin{align*}
 \ev \|S_n\|^2 &\leq \ev \|S_n-T_n+T_n\|^2 \\
 & \leq \ev \|S_n-T_n\|^2+ \ev \| T_n -S_n\| \ev \|T_n\| + \ev\|T_n\|^2 < \infty
\end{align*}
Under the alternative we have 
\begin{align*}
  n^{-1}  \ev | \|T_n\|^2 - \|S_n\|^2 |  &\leq n^{-1}  \ev \big | \|T_n\| - \|S_n\| \big | \big |   \|T_n\| +  \|S_n\| \big | \\
  &\leq \sqrt{ \ev \big | \|T_n\| - \|S_n\| \big |^2   n^{-1}\ev \big| \|T_n\| +  \|S_n\| \big|^2 }\\ 
\end{align*}
it is sufficient to show that  $n^{-1}  \ev\|T_n\|^2< \infty$ and  $n^{-1}  \ev \|S_n\|^2< \infty$. By Theorem \ref{th:mainTwo}, $n^{-1}  \ev \|T_n\|^2< \infty$ is finite and, using the reasoning similar to the one above, we have that    $n^{-1}  \ev \|S_n\|^2< \infty$.
\end{proof}

This shows  that we can use squared norm of $S_n$ as a bootstrapped test statistic.  For HSIC we redefine $B_n$ 
\begin{align}
\label{eq:hsic:1}
B_n^* :=  \| S_n \|^2 = \frac 1  n \sum_{i,j \in N }Q_i, Q_j \tilde k(X_i,X_j) \tilde l(X_i,X_j).
\end{align}
$B_{1}^*$ corresponds to $Q_i=W_i$, $B_{2}^*$  corresponds to $Q_i= \tilde W_i$ . This bootstrapped statistic interestingly coincides with $V_n(h)$. \cite{gretton_kernel_2008} showed that 
\begin{align}
\label{eq:hsic:2}
V_n(h) = \frac 1  n \sum_{i,j \in N }\tilde k(X_i,X_j) \tilde l(X_i,X_j).
\end{align}
Finally, notice that both statistics \ref{eq:hsic:1} and \ref{eq:hsic:2} can be calculated in quadratic time.


\begin{proposition}
\label{prop:null}
Let  $Z_t=\left(X_t,Y_t\right)$  be a stationary process  that is $\tau$-dependent such that $\sum_{r=1}^{\infty} r^2 \sqrt{ \tau(r)} <\infty$. Under the null hypothesis  $B_n^*$ (\ref{eq:hsic:1}) and $n V_n(h)$  (\ref{eq:hsic:2})converge weakly to the same distribution. Under the alternative hypothesis $B_n^*$  converges to zero in probability, while $V_n(h)$ converges to a positive constant. 
\end{proposition}
\begin{proof}
We calculate 
\begin{align*}
 n V_n(h) - B_n^* = n V_n(h) - 6 nB_n(h_2) + 6 nB_n(h_2) -  B_n^* .
\end{align*}
 By Lemma \ref{lem:T_n},  $6 nB_n(h_2) = \| T_n \|^2$. By definition \eqref{eq:hsic:1}, $B_n^* =  \| S_n \|^2 $ .  By Lemma \ref{lem:difS_nT_n}, $6 nB_n(h_2) -  B_n^*$ converges to zero in mean square. We check assumptions; since process $Z_t$ is $\tau$-mixing (of order $o(r^{-4})$ ) and both $\bar k$, $\bar l$ are canonical, Lemma \ref{lem:disentangle} guarantees that  $(\bar k,Z_i)$, $(\bar l,Z_i)$  are of type $\varDelta$ of order $O(r^{-4})$.
 
 Under the null  hypothesis, by Theorem \ref{th:mainOne}, $n V_n(h) - 6 nB_n(h_2)$ converges to zero.  We check assumptions; by Lemma \ref{stm:thanks}, $h_2$ is a symmetric, one-degenerate, bounded kernel, assumptions concerning $\tau$-mixing are satisfied. 
 
 Under the alternative, by Theorem \ref{th:mainTwo} and  Lemma \ref{lem:difS_nT_n} respectively,  $6 B_n(h_2)$ and $\frac 1 n B_n^* -6 B_n(h_2) $ converge to zero in mean square. By Theorem \ref{th:mainTwo}, $V_n(h)$ converges to a positive constant.
 \end{proof}

We consider two types of tests: instantaneous independence and independence at multiple time lags.





%\vspace{-4mm}
\paragraph{Test of instantaneous independence}
%It was shown in \cite{gretton_measuring_2005,gretton_kernel_2008} that for IID random variables $V(h)$ is an estimator of distance between embeddings $ \parallel \mu_{XY} - mu_X \prod mu_y \parallel$. Using \ref{th:mainOne} we will show $V_b(Z)$ is a good estimator in case observations are temporally dependent. The main difficulty however lays in constructing   
Here, the null hypothesis  $\mathbf{H_0}$ is that  $X_t$ and $Y_t$ are independent at all times $t$,  and the alternative hypothesis $\mathbf{H_1}$ is that they are dependent. 

\begin{proposition}
\label{prop:null}
Under the null hypothesis, if the stationary process $Z_t=\left(X_t,Y_t\right)$ is $\tau$-dependent with a coefficient $\tau(r) = O\left(r^{-6-\epsilon}\right)$ for some $\epsilon>0$, then $\varphi(6 n V_b(h),n V(h))\to 0$ in probability, where $\varphi$ is the Prokhorov metric. 
\end{proposition}
\begin{proof}
Since $k$ and $l$ are bounded and Lipschitz continuous, the core $h$ is bounded  and Lipschitz continuous. One-degeneracy under the null hypothesis was stated in \cite[Theorem 2]{gretton_kernel_2008}, and that $h_2$ is a kernel is shown in \cite[section A.2, following eq. (11)]{gretton_kernel_2008}. The result follows from Theorem \ref{th:mainOne}.
\end{proof}

The following proposition holds by the Theorem \ref{th:mainTwo}, since the core $h$ is  Lipschitz continuous, symmetric and bounded.
\begin{proposition}
\label{prop:alternative}
If the stationary process $Z_t$ is $\tau$-dependent with a coefficient $\tau(r) = O\left(r^{-6-\epsilon}\right)$ for some $\epsilon>0$, then under the alternative hypothesis $n V_{b2}(h)$ converges in distribution to some random variable with a finite variance and $ V_{b1}$ converges to zero in probability. 
\end{proposition}


\begin{table}\caption{Rejection rates for two-sample experiments. {\bf MCMC}: sample size=500; a Gaussian kernel with bandwidth
$\sigma=1.7$ is used; every second Gibbs sample is kept (i.e., after a pass
through both dimensions). {\bf Audio}: sample sizes are $(n_x,n_y)=\{(300,200),(600,400),(900,600)\}$; a Gaussian kernel with bandwidth
$\sigma=14$ is used. {\bf Both}: wild bootstrap
uses blocksize of $l_n=20$; averaged over at least 200 trials. The Type II error for all tests was zero}
\label{tab:gibbs_mmd}
\centering{}%
\begin{tabular}{|c|c|c|c|c|c|}
\cline{2-6} 
\multicolumn{1}{c|}{} & {\footnotesize experiment $\backslash$ method} & {\footnotesize permutation} & {\footnotesize $\widehat{\text{MMD}}_{k,b}$} & {\footnotesize $V_{b1}$} & {\footnotesize $V_{b2}$}\tabularnewline
\hline 
\textbf{\scriptsize MCMC} & {\footnotesize i.i.d. vs i.i.d. ($\mathbf{H}_{0}$)} & {\small .040} & {\small .025} & {\small .012}\textbf{\small{} } & {\small .070}\tabularnewline
\cline{2-6} 
 & {\footnotesize i.i.d. vs Gibbs ($\mathbf{H}_{0}$)} & {\small .528 } & {\small .100} & {\small .052} & {\small .105}\tabularnewline
\cline{2-6} 
 & {\footnotesize Gibbs vs Gibbs ($\mathbf{H}_{0}$)} & {\small .680 } & {\small .110} & {\small .060} & {\small .100}\tabularnewline
\hline 
\textbf{\scriptsize Audio} & {\footnotesize $\mathbf{H}_{0}$} & {\small \{.970,.965,.995\}} & {\small \{.145,.120,.114\}} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}\tabularnewline
\cline{2-4} 
 & {\footnotesize $\mathbf{H}_{1}$} & {\small \{1,1,1\}} & {\small \{.600,.898,.995\}} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}\tabularnewline
\cline{1-4} 
\end{tabular}
\end{table}



%\vspace{-4mm}
\paragraph{Lag-HSIC}
Propositions \ref{prop:null} and \ref{prop:alternative} also allow us to construct a test of time series independence that is similar to one designed by  \cite{besserve_statistical_2013}. Here, we will be testing against a broader null hypothesis:  $X_t$ and $Y_{t'}$ are independent for $|t-t'|<M$ for an arbitrary large but fixed $M$. In the Appendix, we show how to construct a test when $M\to\infty$, although this requires an additional assumption about the uniform convergence of cumulative distribution functions.

Since the time series $Z_t=(X_t,Y_t)$ is stationary, it suffices to check whether there exists a dependency between $X_t$ and $Y_{t+m}$ for $-M \leq m \leq M$. Since each lag corresponds to an individual hypothesis, we will require a Bonferroni correction to attain a desired test level $\alpha$. We therefore define $q = 1-\frac{\alpha}{2M+1}$. The shifted time series will be denoted $Z_t^m =(X_t,Y_{t+m})$. Let $S_{m,n}=n V(h,Z^m)$ denote the value of the normalized HSIC statistic calculated on the shifted process $Z_t^m$. Let $F_{b,n}$ denote the empirical cumulative distribution function obtained by the bootstrap procedure using $n V_{b}(h,Z)$. The test will then reject the null hypothesis if the event $\mathcal A_n = \left\{ \max_{-M \leq m \leq M} S_{m,n} > F^{-1}_{b,n}(q) \right\}$ occurs. By a simple application of the union bound, it is clear that the asymptotic probability of the Type I error will be $\lim_{n\to\infty}P_{\,\mathbf{H_0}}\left(\mathcal A_n\right)\leq\alpha$. On the other hand, if the alternative holds, there exists some $m$ with $|m|\leq M$ for which $V(h,Z^m)=n^{-1} S_{m,n}$ converges to a non-zero constant. In this case  
\begin{align}
\label{eg:aletrnative1}
&P_{\,\mathbf{H_1}}(\mathcal A_n)  \geq  P_{\,\mathbf{H_1}}( S_{m,n} > F^{-1}_{b,n}(q)) = P_{\,\mathbf{H_1}}( n^{-1} S_{m,n} > n^{-1} F^{-1}_{b,n}(q) ) \to 1
\end{align}
as long as $n^{-1} F^{-1}_{b,n}(q)\to 0$, which follows from the convergence of $V_{b}$ to zero in probability shown in Proposition \ref{prop:alternative}. Therefore, the Type II error of the multiple lag test is guaranteed to converge to zero as the sample size increases.
%Since  $n^{-1} t$ converges to zero.\\
%These observation result in the following test. We calculate an approximation of the cumulative distribution  . Under the null hypothesis, the Type I error is controlled by the bound from the equation \eqref{eg:null1}. If the alternative holds,  the control of the Type II error follows from the equation \ref{eg:aletrnative1}.\\
Our experiments in the next Section demonstrate that while this procedure is defined over a finite range of lags, it results in tests  more powerful than the procedure for an infinite number of lags proposed in \cite{besserve_statistical_2013}. 
We note that a procedure that works for an infinite number of lags, although possible to construct, does not add much practical value under the present assumptions. Indeed,  since the $\tau$-mixing assumption applies to the joint sequence $Z_t=(X_t,Y_t)$, dependence between $X_t$ and $Y_{t+m}$ is bound to disappear at a rate of $o(m^{-6})$, i.e., the variables both within and across the two series are assumed to become gradually independent at large lags.     
