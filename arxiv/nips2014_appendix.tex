



\section{An Introduction to the Wild Bootstrap}
\label{wildintro}
Bootstrap methods aim to evaluate the accuracy of the sample estimates - they are particularly useful when dealing with complicated distributions, or when the assumptions of a parametric procedure are in doubt. Bootstrap methods randomize the dataset used for the sample estimate calculation, so that a new dataset with a similar statistical properties is obtained, e.g. one popular method is resampling. In the wild bootstrap method  the observations in the dataset are multiplied by  appropriate random numbers. To present the idea behind the wild bootstrap we will discuss a toy example similar to that in \cite{Shao2010}, and then relate it to the wild bootstrap method used in this article. 

Consider a stationary autoregressive-moving-average random process $\{X_i\}_{i \in \mathbf{Z}}$ with zero mean. The normalized sample mean of the process $X_t$ has normal distribution
\begin{equation}
\frac{\sum_{i=1}^{N} X_i}{\sqrt{n}} \overset{d}{\to} N(0,\sigma_{\infty}^{2}),    
\end{equation}      
where $\sigma_{\infty}^2 = \sum_{j=-\infty}^{j=\infty} cov(X_0,X_j)$. The variance $\sigma_{\infty}^2$ is not easy to estimate (the naive approach of approximating different covariances separately and summing them has several drawbacks, e.g. how many empirical covariances should be calculated?). Using the wild bootstrap method we will construct processes that mimic behaviour of the $X_t$ process and then use them to approximate the distribution of the normalized sample mean, $\frac{\sum_{i=1}^{N} X_i}{\sqrt{n}}$. The bootstrap process used to to randomize the sample meets the following criteria: 
\begin{itemize}
\item $\{W_{t,n}\}_{1 \leq t \leq n }$ is a row-wise strictly stationary triangular array independent of all $X_t$, such that $\ev W_{t,n}=0$ and $\sup_{n} \ev|W_{t,n}^{2+\sigma}| < \infty$ for some $\sigma > 0$. 
\item The autocovariance of the process is given by $\ev W_{s,n} W_{t,n}=\rho(|s-t|/l_n)$ for some function $\rho$, such that $\lim_{u \to 0} \rho(u) = 1$. 
\item The sequence $\left\{l_n\right\}$ is taken such that $\lim_{n \to \infty} l_n = \infty$.
\end{itemize}
A process that fulfils those criteria, given also in the main article, is
\begin{align}
W_{t,n} = e^{-1/l_n}W_{t-1,n} + \sqrt{1 -e^{-2/l_n}} \epsilon_t
\end{align} 
  
We need to show that the distribution of the normalized sample mean of the process  $Y_t^{n} = W_t^{n}X_t$, where $|t| \leq n$, mimics the distribution $N(0,\sigma_{\infty}^2)$. It suffices to calculate the expected value and correlations:   
\begin{align}
\ev Y_t^{n} &= \ev W_t^n X_t = 0 ,\\
cov(Y_0^n,Y_t^n) &= cov(X_0,X_t)cov(Y_0^n,Y_t^n) = cov(X_0,X_t)\rho(|t|/l_n) \to cov(X_0,X_t)
\end{align}
The asymptotic auto-covariance structure of the process $Y_t$ is the same as the auto-covariance structure of the process $X_t$. Therefore 
\begin{equation}
\frac{\sum_{i=1}^{N} Y_i}{\sqrt{n}} \overset{d}{\to} N(0,\sigma_{\infty}).    
\end{equation}  

This mechanism is used in \cite{leucht_dependent_2013}. Recall that, under some assumptions, a normalized V-statistic can be written as 
$$
\sum_{k=0}^{\infty} \lambda_k  \left( \frac{ \sum_{i=1}^{n} \phi_k(X_i) } {\sqrt n}  \right)^2 \overset{P}{=} \frac 1  n \sum_{1\leq i,j \leq n} h(X_i,X_j) 
$$ 

where $\lambda_k$ are eigenvalues and $\phi_k$ are eigenfunction of the  kernel $h$, respectively.  Since $\ev  \phi_k(X_i) = 0$ (degeneracy condition) one may replace  
$$  \frac{ \sum_{i=1}^{n} \phi_k(X_i)} {\sqrt n} $$
with a bootstrapped version 
$$ \frac{  \sum_{i=1}^{n}  W_t^n \phi_k(X_i) } {\sqrt n}, $$  
and conclude, as in the toy example, that the limiting distribution of the single component of the sum $\sum_k \lambda_k  ...$  remains the same. One of the main  contributions of \cite{leucht_dependent_2013}  is in showing that the distribution of the whole sum $\sum_k \lambda_k \left(\frac{  \sum_{i=1}^{n}  W_t^n \phi_k(X_i) } {\sqrt n} \right)^2$ with the components bootstrapped  
converges in a particular sense (in  probability in Prokhorov metric) to the distribution of the normalized V-statistic, $\frac 1  n \sum_{1\leq i,j \leq n} h(X_i,X_j) $.


%is the same (in quite a peculiar sense i.e. convergence in  probability in Prokhorov metric) as distribution of the normalized V-statistic, $\frac 1  n \sum_{1\leq i,j \leq n} h(X_i,X_j) $.

\section{Relation between $\beta$,$\phi$ and $\tau$ mixing}\label{append:differentMixing}


\paragraph{Strong mixing coefficients.}\
A process is called absolutely regular ($\beta$-mixing) if $\beta(m) \rightarrow 0$, where 
\begin{equation*}
\beta(m) = \frac 1 2 \sup_n \sup \sum_{i=1}^{I} \sum_{j=1}^{J}  |P(A_i \cap B_j) - P(A_i)P(B_j) |.
\end{equation*}
The second supremum in the $\beta(m)$ definition is taken over all pairs of finite partitions $\{A_1,\cdots,A_I\}$  and $\{B_1,\cdots,B_J\}$ of the sample space such that $A_i \in \mathcal{A}_{1}^{n}$ and $B_j \in \mathcal{A}_{n+m}^{\infty}$, and $\mathcal{A}_{b}^{c}$ is a sigma field spanned by a subsequence, $\mathcal{A}_{b}^{c} = \sigma(Z_b,Z_{b+1}, ..., Z_{c})$. 
A process is called uniform mixing ($\phi$-mixing) if $\phi(m) \rightarrow 0$, where
\begin{equation*}
\phi(m) = \sup_n  \sup_{A \in \mathcal{A}_{1}^{n} } \sup_{B \in \mathcal{A}_{n+m}^{\infty}}  |P(B|A) - P(B)|.
\end{equation*}
The process is called strongly mixing ($\alpha$-mixing) if $\alpha(m) \rightarrow 0$, where
\begin{equation*}
\alpha(m) = \sup_n  \sup_{A \in \mathcal{A}_{1}^{n} } \sup_{B \in \mathcal{A}_{n+m}^{\infty}}  |P(B \cap A) - P(B)P(A)|.
\end{equation*}
By \cite{bradley_basic_2005} we have  $\alpha(m) \leq \beta(m) \leq \phi(m)$ . 

\paragraph{Weak mixing The expected value and variance ofcoefficients.}\
The process is called $\tilde \alpha$-mixing if $\tilde \alpha(m) \rightarrow 0$, where 
\begin{align*}
\tilde \alpha(m)  &= \sup_{l \in \mathbb{N}} \frac 1 l \sup_{ m \leq i_1 \leq ... \leq i_l} \tilde \alpha( \mathcal F_0,(Z_{i_1},...,Z_{i_l}) )  \overset{r \to \infty}{\longrightarrow} 0,\;\text{where} \\
\tilde \alpha(\mathcal{M},X)  &=    \sup_{g \in \Lambda} \parallel  \ev(g(X)|\mathcal{M})  - \ev g(X) \parallel_{1} 
\end{align*}
and $\Lambda$ is the set of all one-Lipschitz continuous real-valued functions on the domain of $X$.
The other weak mixing coefficient, already introduced,  is $\tau$-mixing. \cite[Remark 2.4]{dedecker2007weak} show that $\tilde \alpha(m) \leq 2\alpha(m)$. \cite[Proposition 2]{dedecker2005new} relates $\tau$-mixing and $\tilde \alpha$ mixing, as follows: if $Q_x$ is the generalized inverse of the tail function
\[
 Q_x(u) = \inf_{t \in R} \{  P(|X| > t) \leq u\},  
\]
then
\[
 \tau(\mathcal{M},X) \leq 2 \int_{0}^{\tilde \alpha(\mathcal{M},X)}  Q_x(u) du.
\]
While this definition can be hard to interpret, it can be simplified in the case $E|X|^p=M$  for some $p>1$, since via Markov's inequality $P(|X|>t) \leq \frac{M}{t^p}$, and thus $\frac{M}{t^p} \leq u $ implies $P(|X|>t) \leq u$. Therefore $Q'(u) = \frac{M}{\sqrt[p]{u}} \geq Q_x(u)$. As a result, under the assumption that the real valued random variable is $p$-integrable for some $p>1$, we have the following inequality 
\[
 \frac{ \sqrt[p]{\tilde   \alpha(\mathcal{M},X)}}{M}  \geq C  \tau(\mathcal{M},X) 
\]



\section{Proofs}
The proofs are organized as follows 
\begin{itemize}
\item Subsection \ref{sec:basicNotationandFacts} introduces notation and states basic results concerning $V$-statistics.
\item Subsection \ref{sec:prMainOne} provides proofs of Lemmas \ref{lem:equivVanila} and \ref{lem:equivBoot} that constitute for most of the poof of the Theorem \ref{th:mainOne}.
\item  Subsection \ref{sec:prMainOne} provides proofs of Lemmas \ref{lem:Components} and \ref{lem:degb2} from which Theorem \ref{th:mainTwo} follows.
\item Subsection \ref{sub:prop:mmd}  provides proof of the Proposition \ref{prop:mmd}.
\item Finally, section \ref{label:aux} lists all auxiliary lemmas. 
\end{itemize}

\subsection{Notation and basic facts.}
\label{sec:basicNotationandFacts}

The following Lemma introduces notation and basic facts about $V$-statistics. These can be found can be found in the \cite[Section 5.1.5]{serfling80} (page 178). 
\begin{lemma}
\label{lem:Components}\cite[Section 5.1.5]{serfling80}
Any core $h$ can be written as a sum of canonical cores $h_1,...,h_m$ and a constant $h_0$
\begin{align*}
h(z_1,...,z_m) &=   h_m(z_1,...,z_m) + \sum_{1 \leq i_1 < ...<i_{m-1} \leq m } h_{m-1}(z_{i_1},...,z_{i_{m-1}}) \\ 
    & + ... + \sum_{1 \leq i_1 < i_2 \leq m } h_2(z_{i_1},z_{i_2}) + \sum_{1 \leq i \leq m} h_1(z_i)+h_0
\end{align*} 
We call $h_1,...,h_m$ components of a core $h$. We do not call $h_0$ a component, its simply a constant. The components are defined in terms of auxiliary functions $g_c$
\[
 g_c(z_1,...z_c) = \ev h(z_1,...,z_c,Z_{c+1}^*,...,Z_{m}^*)
\]
for each $c=0,...,m-1$ and we put $g_m=h$. We define components as follows
\begin{align}
   h_0 &= g_0, \\  
   h_1(z_1) &= g_1(z_1) -h_0,\\
   h_2(z_1,z_2) &= g_2(z_1,z_2)  - h_1(z_1) - h_1(z_2)-h_0, \\  
   h_3(z_1,z_2,z_3) &= g_3(z_1,z_2,z_3) - \sum_{1 \leq i < j \leq 3 } h_2(z_i,z_j) - \sum_{1\leq i \leq 3} h_1(z_i)-h_0, \\ 
   \cdots &, \\
   h_m(z_1,...,z_m) &= g_m(z_1,...,z_m) - \sum_{1 \leq i_1 < ...<i_{m-1} \leq m } h_{m-1}(z_{i_1},...,z_{i_{m-1}}) \\ 
    & - ... - \sum_{1 \leq i_1 < i_2 \leq m } h_2(z_{i_1},z_{i_2}) - \sum_{1 \leq i \leq m} h_1(z_i)-h_0.\\
\end{align}
\cite[Section 5.1.5]{serfling80} shows that components $h_c$ are symmetric (and therefore cores) and canonical.  Finally a V-statistic of a core function h can be written as a sum of  V-statistics with canonical cores
\begin{align}
  V(h) = V(h_m) + \binom m 1 V(h_{m-1}) + ...+ \binom {m} {m-2} V(h_{2}) + \binom {m} {m-1} V(h_{1}) + h_0.
 \end{align}
\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%% Proof of the Theorem  \ref{th:mainOne} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Proof of Theorem  \ref{th:mainOne}} 
 \label{sec:prMainOne}
 
\begin{lemma}
\label{lem:equivVanila}
Assume that the stationary process $Z_t$ is $\tau$-dependent with a coefficient $\tau(i) = i^{-6-\epsilon}$ for some $\epsilon>0$. If core $h$ is Lipschitz continuous, one-degenerate, bounded, and its $h_2$ component is a kernel then its normalized $V$ statistic limiting distribution is proportional to its second component normalized $V$-statistic distribution. Shortly    
\begin{align}
\lim_{n \to \infty} \varphi( n V(h), \binom m 2  n V(h_2) ) = 0
\end{align}
where $\varphi$ denotes Prokhorov metric. 
\end{lemma}
\begin{proof}
Lemma \ref{lem:Components} shows how to write core  $h$ as a sum of its components $h_i$ ,
\begin{align}
  n V(h) = n V(h_m) + \binom m 1 n V(h_{m-1}) + ...+ \binom {m} {m-2} n V(h_{2}) + \binom {m} {m-1} n V(h_{1})+h_0.
\end{align}
By Lemma \ref{stm:LipAndBound} all components of $h$ are bounded and Lipschitz continuous. Since $h$ is one-degenerate, $h_0=0$ and component $h_1(z)$ is equal to zero everywhere
\begin{align}
h_1(z) = \ev h(z,Z_{2}^*,...,Z_{m}^*)=0. 
\end{align}

By Lemma \ref{lem:higherVstats2}, for $c \geq 3$, $n V(h_{c})$  converges to zero in probability. Therefore the behaviour of $V(h)$ is determined by $\binom {m} {m-2} V(h_2) = \binom {m} {2} V(h_2)$. Convergence of $V(h_2)$ follows from the \cite[Theorem 2.1]{leucht_dependent_2013}.
\end{proof}




\begin{lemma}
\label{lem:equivBoot}
Assume that the stationary process $Z_t$ is $\tau$-dependent with a coefficient $\tau(i) = i^{-6-\epsilon}$ for some $\epsilon>0$. If core $h$ is Lipschitz continuous, one-degenerate, bounded, and its $h_2$ component is a kernel then its normalized and bootstrapped $V$ statistic limiting distribution is same its second component normalized and bootstrapped $V$-statistic distribution. Shortly
\begin{align}
\lim_{n \to \infty} \varphi( n V_b(h), nV_b(h_2)) =0, 
\end{align}
in probability, where $\varphi$ denotes Prokhorov metric. 
\end{lemma}
\begin{proof}
We show that the proposition holds for $V_{b1}$ and then we prove that $\varphi( n V_{b2}(h), nV_{b1}(h)) =0$ converges to zero - the concept is to \cite{leucht_dependent_2013}.   

\textbf{$nV_{b1}$ convergence.} We write core  $h$ as a sum of components $h_i$ ( $h_0,h_1$ are equal to zero and therefore omitted). By Lemma \ref{lem:Components}
\begin{align}
 n V_{b1}(h)& = \frac{1} {n^{m-1}}  \sum_{i \in N^m}  \Big[ W_{i_1} W_{i_2}   h_m(Z_{i_1},...,Z_{i_m})  + \\ 
 & \sum_{1 \leq j_1 < ...<j_{m-1} \leq m } W_{i_1} W_{i_2} h_{m-1}(Z_{i_{j_1}},...,Z_{i_{j_{m-1}}})   + ... + \sum_{1 \leq j_1 < j_2 \leq m } W_{i_1} W_{i_2} h_2(Z_{i_{j_1}},Z_{i_{j_2}}) \Big].
\end{align}
Consider a sum associated with $h_2$
\begin{align}
\frac{1} {n^{m-1}}  \sum_{i \in N^m}  \sum_{1 \leq j_1 < j_2 \leq m } W_{i_1} W_{i_2} h_2(Z_{i_{j_1}},Z_{i_{j_2}}).
\end{align}
Fix $j_1,j_2$. If $j_1 \neq 1$,  $j_2 \neq 2$ then the sum 
\begin{align}
\label{eq:difference}
&\frac{1} {n^{m-1}}  \sum_{i \in N^m}   W_{i_1} W_{i_2} h_2(Z_{i_{j_1}},Z_{i_{j_2}}) \overset{L.\ref{lem:summingLema}}{=\joinrel=}   \frac{1} {n^3}  \sum_{i \in N^4}   W_{i_1} W_{i_2} h_2(Z_{i_3},Z_{i_4}) = \\
& \left( \frac{1}{n}   \sum_{ i \in N^2} h_2(Z_{i_1},Z_{i_2}) \right) (\frac{1}{n} \sum_{i=1}^{n}W_i)^2 \overset{L. \ref{lem:toZeroWi}}{\longrightarrow} 0 \text{ in probability}.  
\end{align}
If $j_1 = 1$ and  $j_2 \neq 2$, then the sum  
\begin{align}
\label{eq:h2eq1}
&\frac{1} {n^{m-1}}  \sum_{i \in N^m}  W_{i_1} W_{i_2} h_2(Z_{i_{j_1}},Z_{i_{j_2}})  \overset{L.\ref{lem:summingLema}}{=\joinrel=} \frac{1} {n^2}  \sum_{i \in N^3}   W_{i_1} W_{i_3} h_2(Z_{i_1},Z_{i_3}) = \\
& \left( \frac{1}{n} \sum_{i \in N^2} W_{i_1}  h_2(Z_{i_1},Z_{i_2}) \right) \left( \frac 1 n \sum_{i=1}^{n}W_i \right) \overset{L.\ref{lem:meanWi},\ref{lem:oneWtrick}  }{\longrightarrow} 0 \text{ in probability}.
\end{align}
The similar reasoning holds for $j_i=2$ and $j_2>2$. The sum associated with $h_c$ for $c>2$
\begin{align}
\frac{1} {n^{m-1}}  \sum_{i \in N^m}  \sum_{1 \leq j_1 < ... < j_c \leq m } W_{i_1} W_{i_2} h_c(Z_{i_{j_1}},...,Z_{i_{j_c}}) \overset{L. \ref{lem:higherVstats}  }{\longrightarrow} 0  \text{ in probability}.
\end{align}
Therefore 
\begin{align}
\lim_{n \to \infty} \left( n V_b(h) - \sum_{i \in N^2} W_{i_1}W_{i_2} h_2(Z_{i_1},Z_{i_2}) \right) \overset{P}{=}0.
\end{align}
what proofs the proposition for $V_{b1}$.


\textbf{$nV_{b1}$ convergence.} To prove that  $nV_{b2}$ converges to the same distribution as $nV_{b1}$ we investigate the difference
\begin{align}
&V_{b1} - V_{b2} = \frac{1} {n^{m-1}} \sum_{i \in N^m} W_{i_1}W_{i_2} h(Z_{i_1},...,Z_{i_m}) - \frac{1} {n^{m-1}} \sum_{i \in N^m} \tilde W_{i_1} \tilde W_{i_2} h(Z_{i_1},...,Z_{i_m}) = \\
&\frac{1} {n^{m-1}} \sum_{i \in N^m} W_{i_1}W_{i_2} h(\cdot) - \frac{1} {n^{m-1}} \sum_{i \in N^m}  (W_{i_1} -\frac 1 n \sum_{j=1}^n W_j ) (W_{i_2} -\frac 1 n \sum_{j=1}^n W_j ) h(\cdot) = \\
&-\left(\frac 2 n \sum_{j=1}^n W_j \right) \left( \frac{1} {n^{m-1}} \sum_{i \in N^{m}} W_{i_1} h(\cdot) \right)  + \left(\frac{1} {n^{m-1}} \sum_{i \in N^{m}}  h(\cdot) \right) \left(\frac 1 n \sum_{j=1}^n W_j \right)^2.
\end{align} 
The second term
\begin{align}
\left(\frac{1} {n^{m-1}} \sum_{i \in N^{m}}  h(Z_{i_1},...,Z_{i_m}) \right) \left(\frac 1 n \sum_{j=1}^n W_j \right)^2 \overset{L. \ref{lem:toZeroWi}}{\longrightarrow} 0 \text{ in probability}.
\end{align}
Therefore we only need to show that the first term converges to zero
\begin{align}
\label{eq:firstTerm}
\left(\frac 2 n \sum_{j=1}^n W_j \right) \left( \frac{1} {n^{m-1}} \sum_{i \in N^{m}} W_{i_1} h(Z_{i_1},...,Z_{i_m}) \right).
\end{align}
Since $\frac 2 n \sum_{j=1}^n W_j$ converges in probability to zero (by Lemma \ref{lem:meanWi}) we only nee to show that $\frac{1} {n^{m-1}} \sum_{i \in N^{m}} W_{i_1} h(Z_{i_1},...,Z_{i_m})$ converges. Using decomposition from Lemma \ref{lem:Components} we write
\begin{align}
\label{eq:xyz}
&\frac{1} {n^{m-1}} \sum_{i \in N^{m}} W_{i_1} h(Z_{i_1},...,Z_{i_m}) =\frac{1} {n^{m-1}}  \sum_{i \in N^m}  \Big[ W_{i_1}    h_m(Z_{i_1},...,Z_{i_m})  + \\ 
 & \sum_{1 \leq j_1 < ...<j_{m-1} \leq m } W_{i_1}  h_{m-1}(Z_{i_{j_1}},...,Z_{i_{j_{m-1}}})   + ... + \sum_{1 \leq j_1 < j_2 \leq m } W_{i_1}  h_2(Z_{i_{j_1}},Z_{i_{j_2}}) \Big].
\end{align}
Term associated with $h_2$ can be written as
\begin{align}
&\frac{1} {n^{m-1}} \sum_{i \in N^{m}} \sum_{1 \leq j_1 < j_2 \leq m } W_{i_1}  h_2(Z_{i_{j_1}},Z_{i_{j_2}}) = \\
&= \left\{
 \begin{array}{lr}
    n^{-1} \sum_{i \in N^{2}}  W_{i_1}  h_2(Z_{i_1},Z_{i_2}) : j_1=1 \text{ or } j_1=2 \\
    \left( n^{-1} \sum_{i \in N^{2}}   h_2(Z_{i_1},Z_{i_2}) \right) \left( \frac 1 n \sum_{j=1}^n W_j \right) : \text{ otherwise }. 
  \end{array}
\right.
\end{align}
In the first case ($j_1=1$  or $j_1=2$ ) Lemma \ref{lem:oneWtrick} assures convergence. In the second case we use Lemma \ref{lem:toZeroWi} to show convergence to zero. 
Other terms with $h_c$ for $c>2$
\begin{align}
\frac{1} {n^{m-1}}  \sum_{i \in N^m} \sum_{1 \leq j_1 < ...<j_{c} \leq m } W_{i_1}  h_{m-1}(Z_{i_{j_1}},...,Z_{i_{j_{c}}}) \overset{L. \ref{lem:higherVstats}}{\longrightarrow} 0 \text{ in probability.}
\end{align}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%   ALTERNATIVE    %%%%%%%%%%%%%%%%%
\subsection{Proof of Proposition \ref{prop:alternative}}
\label{sec:prMainTwo}



\begin{lemma}
\label{lem:degb1}
$nV_{b2}(h)$ converges to some non-zero random variable with finite variance.
\end{lemma}

\begin{proof}
Using decomposition from the Lemma \ref{lem:Components} we write core  $h$ as a sum of components $h_c$ and $h_0$  
\begin{align}
\label{eq:bootstrapedOne}
 &n V_{b2}(h) = \frac{1} {n^{m-1}}  \sum_{i \in N^m}  \Big[h_0  \tilde W_{i_1} \tilde W_{i_2} + \sum_{1 \leq j \leq m } \tilde W_{i_1} \tilde W_{i_2} h_1(Z_{i_j})    \\ 
 &\sum_{1 \leq j_1 < j_2 \leq m } \tilde W_{i_1} \tilde W_{i_2} h_2(Z_{i_{j_1}},Z_{i_{j_2}}) + ... +  \tilde W_{i_1} \tilde W_{i_2}   h_m(Z_{i_1},...,Z_{i_m}) \Big].
\end{align}
We examine terms of the above sum starting form the one with $h_0$ - it is equal to zero
\begin{align}
\frac{1} {n^{m-1}}  \sum_{i \in N^m}  h_0  \tilde W_{i_1} \tilde W_{i_2}   \overset{L.\ref{lem:summingLema}}{=\joinrel=} \frac 1 n h_0 \sum_{i \in N^2} \tilde W_{i_1} \tilde W_{i_2} = \frac 1 n h_0 \left( \sum_{i=1} \tilde W_i \right)^2  \overset{L.\ref{stmt:obviousD}}{=\joinrel=} 0.
\end{align}  
Term with $h_1$ is zero as well, to see that fix $j$ and consider 
\begin{align}
T_{j} = \frac{1} {n^{m-1}}  \sum_{i \in N^m}  \tilde W_{i_1} \tilde W_{i_2} h_1(Z_{i_j}).  
\end{align}  
If $j=1$ then
\begin{align}
T_{1} \overset{L.\ref{lem:summingLema}}{=\joinrel=} \frac{1} {n}  \sum_{i \in N^2}  \tilde W_{i_1} \tilde W_{i_2} h_1(Z_{i_1}) =  \frac{1} {n}  \left( \sum_{i=1}^n  \tilde W_i h_1(Z_i) \right) \left( \sum_{i=1} \tilde W_i \right) \overset{L.\ref{stmt:obviousD}}{=\joinrel=} 0.
\end{align}
If $j=2$ the same reasoning holds and if $j>2$
\begin{align}
T_{j} \overset{L.\ref{lem:summingLema}}{=\joinrel=} \frac{1} {n^2}  \sum_{i \in N^3}  \tilde W_{i_1} \tilde W_{i_2} h_1(Z_{i_3}) =  \frac{1} {n}  \left( \sum_{i=1}^n h_1(Z_i) \right) \left( \sum_{i=1} \tilde W_i \right)^2 \overset{L.\ref{stmt:obviousD}}{=\joinrel=} 0.
\end{align}
Term containing $h_2$ 
\begin{align}
T_{j_1,j_2} = \frac{1} {n^{m-1}}  \sum_{i \in N^m}  \tilde W_{i_1} \tilde W_{i_2} h_2(Z_{i_{j_1}},Z_{i_{j_2}})
\end{align}
is not zero. In the Lemma \ref{lem:convergenceProblem}  we show that for $j_1=1$ and $j_2=2$ it  converges to some non-zero variable. For $j_1 = 1$ and $j_2 > 2$ we have
\begin{align}
T_{1,j_2} \overset{L.\ref{lem:summingLema}}{=\joinrel=} \frac{1} {n^2}  \sum_{i \in N^3}  \tilde W_{i_1} \tilde W_{i_2} h_2(Z_{i_1},Z_{i_{j_2}}) = \frac{1} {n^2} \left( \sum_{i \in N^2}  \tilde W_{i_1}  h_2(Z_{i_1},Z_{i_2}) \right) \left( \sum_{i=1} \tilde W_i \right)  \overset{L.\ref{stmt:obviousD}}{=\joinrel=} 0.
\end{align}
Exactly the same argument works for $T_{j_2,1}$. If both $j_1 \neq 1$ and $j_2 \neq 2$ then 
\begin{align}
T_{j_1,j_2} \overset{L.\ref{lem:summingLema}}{=\joinrel=} \frac{1} {n^3}  \sum_{i \in N^4}  \tilde W_{i_1} \tilde W_{i_2} h_2(Z_{i_{j_1}},Z_{i_{j_2}}) = \frac{1} {n^3} \left( \sum_{i \in N^2}   h_2(Z_{i_{j_1}},Z_{i_{j_2}})\right) \left( \sum_{i=1} \tilde W_i \right)^2 \overset{L.\ref{stmt:obviousD}}{=\joinrel=}0.
\end{align}   
Terms containing $h_c$ for $c>2$ 
\begin{align}
\frac{1} {n^{m-1}}  \sum_{i \in N^m} \sum_{1 \leq j_1 < ...<j_{c} \leq m } \tilde W_{i_1} \tilde W_{i_2}  h_{m-1}(Z_{i_{j_1}},...,Z_{i_{j_{c}}}) \overset{L. \ref{lem:higherVstats}}{\longrightarrow} 0 
\end{align}
converge to zero in probability.
 \end{proof}
  
\begin{lemma}
\label{lem:degb2}
$ V_{b1}$ converges to zero in probability.  
\end{lemma}  
\begin{proof}
The expected value and variance of $V_{b1}$ converge to 0, therefore $V_{b1}$ converges to zero in probability. Indeed for an expected value we have
\begin{align}
& \ev V_{b1} = \frac {1} {n^m} \sum_{i \in N^m} \ev W_{i_1} W_{i_2} \ev h(Z_{i_1},...,Z_{i_m}) =  \frac {1} {n^m} \sum_{i \in N^m} \rho(|i_2-i_1|/l_n)  \ev h(\cdot) \leq   \\
&\frac {1} {n^m} \sum_{i \in N^m}  \rho(|i_2-i_1|/l_n) \parallel h \parallel_{\infty} =  \parallel h \parallel_{\infty} \frac {1} {n^2} \sum_{i \in N^2}  \rho(|i_2-i_1|/l_n)  \to 0.
\end{align}
Last convergence follows from the fact that $\sum_{i \in N^2}  \rho(|i_2-i_1|/l_n) \leq \sum_{r=1}^{n-1} n \rho(|r|/l_n)= O(n l_n)$. Similar reasoning shows convergence of $\ev V_{b1}^2$.
\end{proof}  
  
  \subsection{Proof of Proposition \ref{prop:mmd} }
  \label{sub:prop:mmd}
\begin{proposition*}
 Let $k$ be bounded and Lipschitz continuous, and let $\left\{ X_t \right\}$ and $\left\{ Y_t \right\}$ 
 both be $\tau$-dependent with coefficients $\tau(i) =  O(i^{-6-\epsilon})$, but independent of each other. Further, let $n_x=\rho_x n$ and $n_y=\rho_y n$ where $n=n_x+n_y$. Then, under the null hypothesis $P_x=P_y$, $\varphi\left(\rho_x \rho_y n\widehat{\text{MMD}}_k, \rho_x \rho_y n\widehat{\text{MMD}}_{k,b}\right)\to 0$ in probability as $n\to\infty$, where $\varphi$ is the Prokhorov metric.
\end{proposition*}
  \begin{proof}
  Since $\widehat{\text{MMD}}_k$ is just the MMD between empirical measures
using kernel $k$, it must be the same as the empirical MMD $\widehat{\text{MMD}}_{\tilde k}$ with centred kernel $\tilde{k}(x,x')=\left \langle k(\cdot,x)-\ev k(\cdot,X), k(\cdot,x')-\ev k(\cdot,X) \right \rangle_{\Hk}$ according to \cite[Theorem 22]{SejSriGreFuk13}. Using the Mercer expansion, we can write
\begin{align*}
\rho_x \rho_y n\widehat{\text{MMD}}_k & = \rho_{x}\rho_{y}n\sum_{r=1}^{\infty}\lambda_{r}\left(\frac{1}{n_{x}}\sum_{i=1}^{n_{x}}\Phi_{r}(x_{i})-\frac{1}{n_{y}}\sum_{j=1}^{n_{y}}\Phi_{r}(y_{j})\right)^{2}\\
 & = \sum_{r=1}^{\infty}\lambda_{r}\left(\sqrt{\frac{\rho_{y}}{n_{x}}}\sum_{i=1}^{n_{x}}\Phi_{r}(x_{i})-\sqrt{\frac{\rho_{x}}{n_{y}}}\sum_{j=1}^{n_{y}}\Phi_{r}(y_{j})\right)^{2},
\end{align*}
where $\{\lambda_r\}$ and $\{\Phi_r\}$ are the eigenvalues and the eigenfunctions of the integral operator $f\mapsto \int f(x)\tilde k(\cdot,x)dP_x(x)$ on $L_2(P_x)$. Similarly as in \cite[Theorem 2.1]{leucht_dependent_2013}, the above converges in distribution to $\sum_{r=1}^\infty \lambda_r Z_r^2$, where $\{Z_r\}$ are marginally standard normal, jointly normal and given by $Z_r=\sqrt{\rho_x}A_r-\sqrt{\rho_y}B_r$. $\{A_r\}$ and $\{B_r\}$ are in turn also marginally standard normal and jointly normal, with a dependence structure induced by that of $\{X_t\}$ and $\{Y_t\}$ respectively. This suggests individually bootstrapping each of the terms $\Phi_{r}(x_{i})$ and $\Phi_{r}(y_{j})$, giving rise to 
\begin{align*}
\widehat{\text{MMD}}_{\tilde k, b}&=\sum_{r=1}^{\infty}\lambda_{r}\left(\frac{1}{n_{x}}\sum_{i=1}^{n_{x}}\Phi_{r}(x_{i})\tilde W_i^{(x)}-\frac{1}{n_{y}}\sum_{j=1}^{n_{y}}\Phi_{r}(y_{j})\tilde W_j^{(y)}\right)^{2}\\
{}&=\quad\frac{1}{n_x^2}\sum_{i=1}^{n_x}\sum_{j=1}^{n_x}\tilde W_i^{(x)}\tilde W_j^{(x)}\tilde k(x_i,x_j)-\frac{1}{n_x^2}\sum_{i=1}^{n_y}\sum_{j=1}^{n_y}\tilde W_i^{(y)}\tilde W_j^{(y)}\tilde k(y_i,y_j)\\
{}&\qquad-\frac{2}{n_x n_y}\sum_{i=1}^{n_x}\sum_{j=1}^{n_y}\tilde W_i^{(x)}\tilde W_j^{(y)}\tilde k(x_i,y_j). 
\end{align*}
Now, since $\tilde k$ is degenerate under the null distribution, the first two terms (after appropriate normalization) converge in distribution to $\rho_x\sum_{r=1}^\infty \lambda_r A_r^2$ and  $\rho_y\sum_{r=1}^\infty \lambda_r B_r^2$ by \cite[Theorem 3.1]{leucht_dependent_2013} as required. 
The last term follows the same reasoning - it suffices to check part (b) of \cite[Theorem 3.1]{leucht_dependent_2013} (which is trivial as processes $\left\{ X_t \right\}$ and $\left\{ Y_t \right\}$ are assumed to be independent of each other) and apply the continuous mapping theorem to obtain convergence to $-2\sqrt{\rho_x\rho_y}\sum_{r=1}^\infty \lambda_r A_rB_r$ implying that $\widehat{\text{MMD}}_{\tilde k, b}$ has the same limiting distribution as $\widehat{\text{MMD}}_{k}$.
While we cannot compute $\tilde k$ as it depends on the underlying probability measure $P_x$, it is readily checked that due to the empirical centering of processes $\{\tilde W_t^{(x)}\}$ and $\{\tilde W_t^{(y)}\}$, $\widehat{\text{MMD}}_{\tilde k, b}=\widehat{\text{MMD}}_{k, b}$ holds and the claim follows. Note that the result fails to be valid for wild bootstrap processes that are not empirically centred.
\end{proof}


\subsection{Auxiliary results }
\label{label:aux}
The following section lists all auxiliary lemmas. 

\begin{proposition}{\cite[p.259, Equation 2.1]{leucht_dependent_2013}}
\label{prop:Coupling}
If process  $\{Z_t,\mathcal{F}_t\}_{t \in \mathbb{N}}$  is $\tau$-dependent and $\mathcal{F}$ is rich enough (see \cite[Lemma 5.3]{dedecker2007weak}), then there exists, for all $t<t_1<...<t_l$, $l \in \mathbb N$, a random vector $(Z_{t_1}^*,...,Z_{t_l}^*)$ that is independent of $\mathcal F_t$, has the same distribution  as $(Z_{t_1},...,Z_{t_l})$ and 
\begin{align*}
\ev \norm{(Z_{t_1}^*,...,Z_{t_l}^*) - (Z_{t_1},...,Z_{t_l})}_1 \leq l \tau(t_1-t).
\end{align*}
\end{proposition}

\begin{lemma}
\label{lem:complicated}
Let $\{Z_i,\mathcal{F}_i\}$ be a $\tau$-mixing sequence,$\{ \delta_i \}$ a sequence of $i.i.d$ random variables independent of filtration $\mathcal{F}$, $(i_1\leq ... \leq i_m)$ a non-deceasing sequence of positive integers, $k$ a positive integer such that $1 < k < m$ and $(Z_{i_1},...,Z_{i_m})$ some random vector. Further let $A = (Z_{i_1},...,Z_{i_{k-1}})$, $B= Z_{i_k}$ and $C=(Z_{i_{k+1}},...,Z_{i_{m}})$,  $\mathcal{F_A} =\mathcal{F}_{k-1}$. Then, there exist the independent random variables $B^*$ and $C^*$, both independent of $\mathcal{F_A}$, such that 
\begin{align}
\ev |B-B^*| = \tau(i_{k} -i_{k+1}) \text{ and } \frac{1} {m-k} \ev \parallel C-C^* \parallel_1 \leq \tau(i_{k+1}-i_k) 
\end{align}    
\end{lemma}


\begin{proof}
Let $\mathcal{F_B}  =\mathcal{F}_{k}$. We first use  \cite[Equation 2.1]{leucht_dependent_2013}] (also \cite[Lemma 5.3]{dedecker2007weak}) to construct $C^*$ such that $\frac{1} {m-k}  \ev \parallel C-C^* \parallel_1 \leq  \tau(i_{k+1}-i_k)$. By construction $C^*$ is independent of $\mathcal{F_B}$. Since $ \mathcal{F_A} \subset \mathcal{F_B}$ and $ \sigma(B) \subset \mathcal{F_B}$ and $C^* \indep \sigma(\delta_k) $, then $C^* \indep  (\mathcal{F_A} \vee \sigma(B)  \vee \sigma(\delta_k)  )$. Next by \cite[Lemma 5.2]{dedecker2007weak} we construct $B^*$  such  that $\ev |B-B^*| = \tau(i_{k+1} -i_{k})$, and $B^*$  independent  of $\mathcal{F_A}$, but $\mathcal{F_A} \vee \sigma(B) \vee \sigma(\delta_k)$ measurable. Since   $\sigma(C^*) \indep (\mathcal{F_A} \vee \sigma(B) \vee \sigma(\delta)) $ then $C^*$ and $B^*$ are independent. Finally both $C^*$ and $B^*$ are independent of $\mathcal{F_A}$    
\end{proof}


 
 
 
\begin{lemma}
\label{stm:LipAndBound}
 If $h$ is bounded and Lipschitz continuous core then its components are also bounded and Lipschitz continuous.
\end{lemma}
\begin{proof}
 Note that 
\begin{align}
g_c(z_1,...z_c) = \ev h(z_1,...,z_c,Z_{c+1}^*,...,Z_{m}^*) \leq \ev \parallel h \parallel_{\infty}.  
\end{align}
To prove boundedness  we use induction - we assume that components with low index are bounded and use the fact that sum of bounded functions is bounded to obtain the required results.

We prove Lipschitz continuity similarly, first by showing that $g_c(z_1,...z_c)$ are Lipschitz continuous with the same coefficient as the core $h$ and then we use the fact that sum of Lipschitz continuous functions  is  Lipschitz continuous.
\end{proof}


\begin{lemma}
\label{lem:summingLema}
Let $f$ be a symmetric function and let $j=\{j_1,\ldots,j_q\}$ be a subset of $\{1,\ldots,m\}$. Then
\begin{align}
\sum_{i \in N^m} f(Z_{i_{j_1}},...,Z_{i_{j_q}}) = n^{m-q} \sum_{i \in N^q} f(Z_{i_1},...,Z_{i_q})
\end{align}
\end{lemma}
\begin{proof}
Denote $r=\{r_1,\ldots,r_{m-q}\}=\{1,\ldots,m\}\backslash j$. Then
\begin{align}
{}&\sum_{i \in N^m} f(Z_{i_{j_1}},...,Z_{i_{j_q}}) = \sum_{\left( i_{j_1},...,i_{j_q} \right) \in N^q}  \sum_{\left( i_{r_1},...,i_{r_{m-q}}\right) \in N^{m-q}} f(Z_{i_{j_1}},...,Z_{i_{j_q}}) \notag \\
\qquad&=\sum_{\left( i_{j_1},...,i_{j_q} \right) \in N^q}  \left( f(Z_{i_{j_1}},...,Z_{i_{j_q}})  \sum_{\left( i_{r_1},...,i_{r_{m-q}}\right) \in N^{m-q}} 1 \right) \notag \\
\qquad&= n^{m-q} \sum_{\left( i_{j_1},...,i_{j_q} \right) \in N^q}   f(Z_{i_{j_1}},...,Z_{i_{j_q}})  =n^{m-q} \sum_{i \in N^q} f(Z_{i_1},...,Z_{i_q})\notag.
\end{align}
\end{proof}

\begin{lemma}
\label{lem:boundLemma}
Let $\left\{ Z_{t}\right\} $ be a $\tau$-dependent stationary process
with $\tau(r)=O(r^{-6-\epsilon}).$ Let $h$ be a bounded Lipschitz
continuous function of $m\geq3$ arguments (not necessarily symmetric)
such that $\forall j\in\left\{ 1,\ldots,m\right\} $, 
\begin{equation}
\mathbb{E}h(z_{1},\ldots,z_{j-1},Z_{j},z_{j+1},\ldots,z_{m})=0.\label{eq: canonical_nonsymmetric}
\end{equation}
Then, 
\begin{eqnarray*}
\sum_{i\in[n]^{m}}\left|\mathbb{E}h\left(Z_{i_{1}},\ldots,Z_{i_{m}}\right)\right| & = & O\left(n^{\left\lfloor \frac{m}{2}\right\rfloor }+n^{2\left\lfloor \frac{m}{2}\right\rfloor -4-\epsilon}\right).
\end{eqnarray*}
\end{lemma}

\begin{proof} 
The proof uses the same technique as  \cite[Lemma 3]{arcones1998law}.
 We will focus on ordered $m$-tuples $1\leq i_{1}\leq\ldots\leq i_{m}\leq n$,
and by considering all possible permutations of their indices, we
obtain an upper bound 
\begin{eqnarray}
\sum_{i\in[n]^{m}}\left|\ev h\left(Z_{i_{1}},\ldots,Z_{i_{m}}\right)\right| & \leq & \sum_{1\leq i_{1}\leq\ldots\leq i_{m}\leq n}\sum_{\pi\in S_{m}}\left|\ev h\left(Z_{i_{\pi(1)}},\ldots,Z_{i_{\pi(m)}}\right)\right|,\label{eq: ABCLemma_firstInequality}
\end{eqnarray}
where (strict) inequality stems from the fact that the $m$-tuples
$i$ with some coinciding entries appear multiple times on the right.
Now denote $s=\left\lfloor \frac{m}{2}\right\rfloor +1$ and
\[
j_{1}=i_{2}-i_{1};\; j_{l}=\min\left\{ i_{2l}-i_{2l-1},i_{2l-1}-i_{2l-2}\right\} ,\; l=2,\ldots,s-1;\;\; j_{s}=i_{m}-i_{m-1}.
\]
Let $w(i)=\max\left\{ j_{1},\ldots,j_{s}\right\} $, i.e., $w(i)$
corresponds to the largest minimum gap between an individual entry
in the ordered $m$-tuple $i$ and its neighbours. For example, $w\left(\left[1,2,5,9,9\right]\right)=3$.
Note that $w(i)=0$ means that no entry in $i$ appears exactly once.
Let us assume that the maximum $w(i)=w>0$ is obtained at $j_{r}$
for some $r\in\left\{ 1,\ldots,s\right\} $. Let us partition the
vector $\left(Z_{i_{1}},\ldots,Z_{i_{m}}\right)$ into three parts:
\begin{eqnarray*}
A & = & \left(Z_{i_{1}},\ldots,Z_{i_{2r-2}}\right),\; B=Z_{i_{2r-1}},\; C=\left(Z_{i_{2r}},\ldots,Z_{i_{m}}\right).
\end{eqnarray*}
Note that if $r=1$, $A$ is empty and if $r=s$ and $m$ is odd,
$C$ is empty but this does not change our arguments below. Using
Lemma 6, we can construct $B^{*}$ and $C^{*}$ that are independent
of $A$ and independent of each other and 
\begin{equation}
\ev \left\Vert \left(A,B,C\right)-\left(A,B^{*},C^{*}\right)\right\Vert _{1}\leq m\tau\left(w\right).\label{eq: ABCLemma_property1}
\end{equation}
Because $B^{*}$ consists of a singleton and is independent of both
$A$ and $C^{*}$, (\ref{eq: canonical_nonsymmetric}) implies 
\begin{equation}
\ev h(A,B^{*},C^{*})=0.\label{eq: ABCLemma_property2}
\end{equation}
Thus, for $w(i)=w>0$, we have that
\begin{eqnarray*}
\left|\ev h\left(Z_{i_{1}},\ldots,Z_{i_{m}}\right)\right| & \leq & \ev \left|h\left(A,B,C\right)-h\left(A,B^{*},C^{*}\right)\right|+\left|\ev h(A,B^{*},C^{*})\right|\\
 & \leq & \text{Lip}(h)\ev \left\Vert \left(A,B,C\right)-\left(A,B^{*},C^{*}\right)\right\Vert _{1}+0\\
 & \leq & m\text{Lip}(h)\tau(w).
\end{eqnarray*}
Finally, if the entries within the ordered $m$-tuple $i$ are permuted,
$L_{1}$-norm in (\ref{eq: ABCLemma_property1}) remains the same
and (\ref{eq: ABCLemma_property2}) still holds, so also $\left|\ev h\left(Z_{i_{\pi(1)}},\ldots,Z_{i_{\pi(m)}}\right)\right|\leq m\text{Lip}(h)\tau(w)$
$\forall\pi\in S_{m}$ and 
\[
\sum_{\pi\in S_{m}}\left|\ev h\left(Z_{i_{\pi(1)}},\ldots,Z_{i_{\pi(m)}}\right)\right|\leq m!m\text{Lip}(h)\tau(w).
\]
Let us upper bound the number of ordered $m$-tuples $i$ with $w(i)=w$.
$i_{1}$ can take $n$ different values, but since $i_{2}\leq i_{1}+w$,
$i_{2}$ can take at most $w+1$ different values. For $2\leq l\leq s-1$,
since $\min\left\{ i_{2l}-i_{2l-1},i_{2l-1}-i_{2l-2}\right\} \leq w$,
we can either let $i_{2l-1}$ take up to $n$ different values and
let $i_{2l}$ take up to $w+1$ different values (if $i_{2l}-i_{2l-1}\leq i_{2l-1}-i_{2l-2}$)
or let $i_{2l-1}$ take up to $w+1$ different values and let $i_{2l}$
take up to $n$ different values (if $i_{2l}-i_{2l-1}>i_{2l-1}-i_{2l-2}$),
upper bounding the total number of choices for $\left[i_{2l-1},i_{2l}\right]$
by $2n(w+1)$. Finally, the last term $i_{m}$ can always have at
most $w+1$ different values. This brings the total number of $m$-tuples
with $w(i)=w$ to at most $2^{\ensuremath{s-2}}n^{s-1}(w+1)^{s}$.
Thus, the number of $m$-tuples with $w(i)=0$ is $O(n^{s-1})$ and
since $h$ is bounded, we have
\begin{eqnarray*}
 &  & \sum_{1\leq i_{1}\leq\ldots\leq i_{m}\leq n}\sum_{\pi\in S_{m}}\left|\ev h\left(Z_{i_{\pi(1)}},\ldots,Z_{i_{\pi(m)}}\right)\right|\\
 &  & \qquad\leq O(n^{s-1})+\sum_{w=1}^{n-1}\;\sum_{\underset{w(i)=w}{i\in[n]^{m}}:}\;\sum_{\pi\in S_{m}}\left|\ev h\left(Z_{i_{\pi(1)}},\ldots,Z_{i_{\pi(m)}}\right)\right|\\
 &  & \qquad\quad\leq O(n^{s-1})+2^{\ensuremath{s-2}}m!m\text{Lip}(h)n^{s-1}\sum_{w=1}^{n-1}(w+1)^{s}\tau(w)\\
 &  & \qquad\quad\quad\leq O(n^{s-1})+Cn^{s-1}\sum_{w=1}^{n-1}w^{s-6-\epsilon}\\
 &  & \qquad\quad\quad\quad\leq O(n^{s-1})+O(n^{2s-6-\epsilon}),
\end{eqnarray*}
which proves the claim. We have used $\tau(w)=O(w^{-6-\epsilon})$
and collated the constants into $C$. 
\end{proof}

\begin{lemma}
\label{lem:auxAsymp1}
Assume that the stationary process $Z_t$ is $\tau$-dependent with a coefficient $\tau(i) = i^{-6-\epsilon}$ for some $\epsilon>0$. If $h$ is a canonical and Lipschitz continuous core of three or more arguments, then 
\begin{align}
 \lim_{n \to \infty} \frac{1}{n^{m-1}} \sum_{i \in N^{m}} |\ev   h(Z_{i_1},...,Z_{i_m})| = 0.
\end{align}
\end{lemma}

\begin{proof}
We use Lemma \ref{lem:boundLemma} to obtain a bound 
\begin{align}
\sum_{i \in N^{m}} |\ev   h(Z_{i_1},...,Z_{i_m})| = O\left(n^{\left\lfloor \frac{m}{2}\right\rfloor }+n^{2\left\lfloor \frac{m}{2}\right\rfloor-4-\epsilon}\right).
\end{align} 
The right hand side of the above equation divided by $n^{m-1}$ converges to zero.      
\end{proof}




\begin{lemma}
\label{lem:auxAsymp2}
Assume that the stationary process $Z_t$ is $\tau$-dependent with a coefficient $\tau(i) = i^{-6-\epsilon}$ for some $\epsilon>0$. If $h$ is a canonical and Lipschitz continuous core of three or more arguments, then
\begin{align}
\lim_{n \to \infty}\frac{1} {n^{2m-2}}   \sum_{i \in N^{2m}} \ev |h(Z_{i_1},...,Z_{i_m})h(Z_{i_{m+1}},...,Z_{i_{2m}})| = 0.
\end{align}
\end{lemma}
\begin{proof}
Let $g(z_{i_1},...,z_{i_m},z_{i_{m+1}},...,z_{i_{2m}})=h(z_{i_1},...,z_{i_m})h(z_{i_{m+1}},...,z_{i_{2m}})$. Since $g$ meets assumptions of the Lemma \ref{lem:boundLemma} and for $m \geq 3 $ 
\begin{equation}
\lim_{n \to \infty} \frac{1} {n^{2m-2}} \left(n^m + n^{2m -4-\epsilon} \right) =0 ,
\end{equation}
the Lemma follows from the lemma \ref{lem:boundLemma}.
\end{proof}





\begin{lemma}
\label{lem:auxAsymp2a}
Assume that the stationary process $Z_t$ is $\tau$-dependent with a coefficient $\tau(i) = i^{-6-\epsilon}$ for some $\epsilon>0$. Let $h$ be a canonical and Lipschitz continuous core of $c$ arguments, $3 \leq c \leq m$, and $1 \leq j_1 < ...<j_c \leq m$ be a sequence of $c$ integers. If $Q_{i_1,...,i_m}$ is a random variable independent of $(Z_{i_1},...,Z_{i_m})$ such that $\sup_{i \in N^m} \ev |Q_i| \leq \infty$ and $\sup_{i \in N^{m}} \sup_{o \in N^m} \ev |Q_i Q_o| \leq \infty$  then 
\begin{align}
&\lim_{n \to \infty}\frac{1} {n^{m-1}} \ev  \sum_{i \in N^{m}}  Q_i h(Z_{i_{j_1}},...,Z_{i_{j_c}}) \overset{P}{=} 0. \\
&\lim_{n \to \infty}\frac{1} {n^{2m-2}} \ev  \sum_{i \in N^{m}} \sum_{o \in N^{m}}  Q_i Q_o h(Z_{i_{j_1}},...,Z_{i_{j_c}}) h(Z_{i_{o_1}},...,Z_{i_{o_c}}) \overset{P}{=} 0. \\
\end{align}
For the first limit notice  that 
\begin{align}
&\frac{1} {n^{m-1}} \ev  \sum_{i \in N^{m}}  Q_i h(Z_{i_{j_1}},...,Z_{i_{j_c}}) \leq  \frac{1} {n^{m-1}}  \sum_{i \in N^{m}}  |\ev Q_i| |\ev h(Z_{i_{j_1}},...,Z_{i_{j_c}})| \overset{ \ref{lem:summingLema}}{\leq} \\
& \sup_{i \in N^m} |\ev Q_i|   \frac{1} {n^{c-1}}  \sum_{i \in N^c} |\ev h(Z_{i_1},...,Z_{i_c})| \overset{\ref{lem:auxAsymp1}}{\longrightarrow} 0 \text{  in probability}. 
\end{align}
Similar reasoning, which uses Lemma \ref{lem:auxAsymp2} instead of \ref{lem:auxAsymp1}, shows convergence of the second limit. 
\end{lemma}


\begin{lemma}
\label{lem:higherVstats}
Assume that the stationary process $Z_t$ is $\tau$-dependent with a coefficient $\tau(i) = i^{-6-\epsilon}$ for some $\epsilon>0$. Let $h$ be a canonical and Lipschitz continuous core of $c$ arguments, $3 \leq c \leq m$. If $Q_{i_1,...,i_m}$ is a random variable independent of $(Z_{i_1},...,Z_{i_m})$ such that $\sup_{i \in N^m} \ev |Q_i| \leq \infty$ and $\sup_{i \in N^{m}} \sup_{o \in N^m} \ev |Q_i Q_o| \leq \infty$  then 
\begin{align}
\lim_{n \to \infty} \frac {1} {n^{m-1}} \sum_{i \in N^m}  \sum_{1 \leq j_1<...<j_c < m} \ev Q_i   h_c(Z_{i_{j_1}},...,Z_{i_{j_c}}) \overset{P}{=} 0.
\end{align}
\end{lemma}
\begin{proof}
For each sequence such that  $1 \leq j_1<...<j_c < m$  we apply Lemma \ref{lem:auxAsymp2a} and  conclude that the random sum 
\begin{align}
\frac {1} {n^{m-1}} \sum_{i \in N^m} \ev Q_{i_{j_1},...,i_{j_c}}   h_c(Z_{i_{j_1}},...,Z_{i_{j_c}})
\end{align}
converges to zero in a probability - from this the proposition follows.
\end{proof}


\begin{lemma}
\label{lem:higherVstats2}
Assume that the stationary process $Z_t$ is $\tau$-dependent with a coefficient $\tau(i) = i^{-6-\epsilon}$ for some $\epsilon>0$. If $h$ if a one-degenerate and Lipschitz continuous core of three or more arguments  
\begin{align}
\lim_{n \to \infty} n V(h_c) = 0,
\end{align}
for $2< c \leq m$.
\end{lemma}
\begin{proof}
For each $c$ satisfying $2< c \leq m$, $h_c$ is canonical and  Lipschitz continuous. It suffices to put $Q=1$ and use Lemma \ref{lem:higherVstats}.
\end{proof}





\begin{lemma}
\label{lem:meanWi}
If $W_i$ is a bootstrap process then
\begin{align}
\lim_{n \to \infty} \frac 1 n \sum_{i=1}^n W_i \overset{P}{=} 0.
\end{align}
\end{lemma}
\begin{proof}
By the definition of $W_i$, $\ev (\sum_{i=1}^n W_i)^2 = O(n l_n)$,  $\lim_{n \to \infty} \frac {l_n}{n} =0 $ and $\ev \sum_{i=1}^n W_i = 0$. Therefore $\frac{1} {n} \sum_{i=1}^{n}W_i$ converges to zero in probability.
\end{proof}





\begin{lemma}
\label{lem:toZeroWi}
Assume that the stationary process $Z_t$ is $\tau$-dependent with a coefficient $\tau(i) = i^{-6-\epsilon}$ for some $\epsilon>0$ and $W_i$ is a bootstrap process. Let $f$ be a one-degenerate, Lipschitz continuous, bounded core of at least $m$ arguments, $m \geq 2$. Further assume that $f_2$ is a kernel. Then for a positive integer $p$
\begin{align}
\lim_{n \to \infty } n V(f) \left( \frac 1 n \sum_{i=1}^n W_i \right)^p \overset{P}{=} 0
\end{align}
\end{lemma}

\begin{proof}
By  the Lemma \ref{lem:meanWi} $\frac{1} {n} \sum_{i=1}^{n}W_i$ converges to zero in probability. By Theorem \ref{th:mainOne} $ \frac {1}{n^{m-1}} \sum_{i \in N^m} f(Z_{i_m},...,Z_{i_m}) $ converges to some random variable. 
\end{proof}

\begin{lemma}
\label{lem:convergence2012}
Assume that the stationary process $Z_t$ is $\tau$-dependent with a coefficient $\tau(i) = i^{-6-\epsilon}$ for some $\epsilon>0$ and $W_i$ is a bootstrap process. Let $x=(w,z)$ and suppose $f(x_1,x_2) = g(w_1)g(w_2) h(z_1,z_2)$ where $g$ is Lipschitz continuous, $\ev |g(W_1)|^k \leq \infty$ for any finite $k$ and $h$ is symmetric, Lipschitz continuous, degenerate and bounded. Then 
\begin{align}
n V(f)= \frac 1 n \sum_{i,j} f(X_i,X_j) = \frac 1 n \sum_{i,j} g(W_i) g(W_j) h(Z_i,Z_j) 
\end{align}
converges in law to some random variable.   
\end{lemma}
\begin{proof}
We use  \cite[Theorem 2.1]{leucht2012degenerate} to show that $n V(f)$ converges. We check the assumptions $\textit{A1 - A3}$ 

\textit{Assumption A1.} Point (i) requires that the process $(W_n,Z_n)$ is a strictly stationary sequence of $\mathbb{R}^d$-values integrable random variables - this follows from the assumptions of this Lemma. For the point (ii) we put $\delta=\frac 1 3$ and check condition
\begin{align}
\sum_{r=1}^{\infty} r \tau(r)^{\delta} \leq \sum_{r=1}^{\infty} r r^{-6 \frac 1 3} = \sum_{r=1}^{\infty} r^{-2} < \infty. 
\end{align}

\textit{Assumption A2.} Point (i) requires that the function $f$ is symmetric, measurable and
degenerate. Symmetry and measurability are obvious and so we check degeneracy condition
\begin{align}
\ev g(W_1) g(w) h(Z_1,z) =  \ev g(W_1) g(w) \ev h(Z_1,z) = 0.
\end{align}  

 Point (ii) requires that for $\nu > (2-\delta)/(1-\delta) = 2.5$ (since we have chosen $\delta=\frac 1 3$).
\begin{align}
\sup_{k \in \mathbf{N}} \ev |f(X_1,X_k)|^{\nu} < \infty \text{ and } \sup_{k \in \mathbf{N}} \ev |f(X_1,X_k^*)|^{\nu} < \infty
\end{align}
Both requirements are met since $h$ is bounded and the process $\ev |g(W_i)|^k \leq \infty$ for any finite $k$.

\textit{Assumption A3.} Function $f$ is Lipschitz continuous - this is met since both $g$ and $h$ are Lipschitz continuous.   
\end{proof}



\begin{lemma}
\label{lem:oneWtrick}
Assume that the stationary process $Z_t$ is $\tau$-dependent with a coefficient $\tau(i) = i^{-6-\epsilon}$ for some $\epsilon>0$ and $W_i$ is a bootstrap process. If $h$ is a Lipschitz continuous, degenerate and bounded core of two arguments then 
\begin{align}
\frac{1}{n} \sum_{i \in N^2} W_{i_1} h(Z_{i_1},Z_{i_2}) 
\end{align}
converges in distribution to some random variable.
\end{lemma}
\begin{proof}
\begin{align}
& \frac{1}{n} \sum_{i \in N^2} W_{i_1}  h(Z_{i_1},Z_{i_2}) = \frac 1 4 (V_{+}-V_{-}) \text{ where,} \\   
V_{-} &= n^{-1} \sum_{i \in N^2} (W_{i_1}-1)h(Z_{i_1},Z_{i_2})(W_{i_2}-1), \\
V_{+} &= n^{-1} \sum_{i \in N^2}  (W_{i_1}+1)h(Z_{i_1},Z_{i_2})(W_{i_2}+1),
\end{align}
are normalized V statistics that converge. To see that we use Lemma \ref{lem:convergence2012} with $g_{+}(x)=x+1$ and $g_{-}(x)=x-1$ respectively. The only non-trivial assumption is that $\ev|g_{+}(W_i)|^k < \infty$ and $\ev|g_{-}(W_i)|^k<\infty$  - this follows from $\ev |W_i|^k$. 
\end{proof}







\begin{lemma}
\label{stmt:obviousD}
If $\{W_i\}$ is a bootstrap process then
\begin{align}
\sum_{i=1}^n \tilde W_i = \sum_{i=1}^n  \left( W_i - \frac 1 n \sum_{j=1}^n  W_j \right) = 0. 
\end{align}
\end{lemma}

\begin{lemma}
\label{lem:convergenceProblem}
Assume that the stationary process $Z_t$ is $\tau$-dependent with a coefficient $\tau(i) = i^{-6-\epsilon}$ for some $\epsilon>0$ and $W_i$ is a bootstrap process. If $f$ is canonical, Lipschitz continuous, bounded core then a random variable 
\begin{align}
\frac 1 n  \sum_{1 \leq i,j \leq n} \tilde W_i \tilde W_j f(Z_i,Z_j)
\end{align}
converges in law.
\end{lemma}

\begin{proof}
\begin{align}
&\frac 1 n \sum_{1 \leq i,j \leq n} \tilde W_i \tilde W_j f(Z_i,Z_j) = \frac 1 n \sum_{1 \leq i,j \leq n} \left( W_i - \sum_{a=1}^n W_a \right) \left( W_i - \sum_{b=1}^n W_b \right) f(Z_i,Z_j) =\\
& \quad \quad \frac 1 n    \sum_{1 \leq i,j \leq n} W_i W_j f(Z_i,Z_j)  - \left( \frac 2 n \sum_{1 \leq i,j \leq n} f(Z_i,Z_j) \right) \left( \frac 1 n \sum_{b=1}^n W_b \right)  + \\
& \quad \quad\quad \quad \left( \frac 1 n   \sum_{1 \leq i,j \leq n} f(Z_i,Z_j) \right) \left( \frac 1 n \sum_{b=1}^n W_b \right)^2.
\end{align}
Last two terms converge to zero since by Lemma \ref{stmt:obviousD} $\left( \frac 1 n \sum_{b=1}^n W_b \right)$ converges to zero and  by the Lemma \ref{lem:convergence2012} (with $g=1$ ) $\frac 1 n  \sum_{1 \leq i,j \leq n} f(Z_i,Z_j)$ converges in law. The first term converges by the Lemma \ref{lem:convergence2012}. 
\end{proof}




\section{Lag-HSIC with $M \to \infty$}

We here consider a multiple lags test described in Section \ref{sec:hsic} where the number of lags $M=M_n$ being considered goes to infinity with the sample size $n$. Thus, we will be testing if there exists a dependency between $X_t$ and $Y_{t+m}$ for $-M_n \leq m \leq M_n$ where $\{M_n\}$ is an increasing sequence of positive numbers such that $M_n=o(n^r)$ for some $0<r\leq 1$, but $\lim_{n\to\infty}M_n=\infty$. 
%With increasing sample size $n$, we cover a wider range of lags. Since each lag corresponds to an individual hypothesis, we will require a multiple hypothesis testing correction to attain a desired test level $\alpha$. 
We denote $q_{n} = 1-\frac{\alpha}{2M_n+1}$. As before, the shifted time series will be denoted $Z_t^m =(X_t,Y_{t+m})$ and $S_{m,n}=n V(h,Z^m)$ and $F_{b,n}$ is the empirical cumulative distribution function obtained from $n V_b(h,Z)$. We also let $F_n$ and $F$ denote respectively the finite-sample and the limiting distribution under the null hypothesis of $S_{0,n} = n V(h,Z)$ (or, equivalently, of any $S_{m,n}$ since the null hypothesis holds).

Let us assume that we have computed the empirical $q_{n}$-quantile based on the bootstrapped samples, denoted by $t_{b,q_{n}}=F_{b,n}^{-1}(q_n)$. The null hypothesis is then be rejected if the event $\mathcal A_{n} = \left\{ \max_{-M_n \leq k \leq M_n} S_{m,n} > t_{b,q_{n}} \right\}$ occurs. By definition, since $F$ is continuous, $F_n(x)\to F(x)$, $\forall x$. In addition, our Theorem \ref{th:mainOne} implies that $F_{b,n}(x)\to F(x)$ in probability. Thus, $|F_{b,n}(x)-F_n(x)|\to 0$ in probability as well. However, in order to guarantee that $|q_n-F_n(t_{b,q_{n}})|\to 0$, which we require for the Type I error control, we require a stronger assumption of uniform convergence, that $\left\Vert F_{b,n}-F_n\right\Vert_{\infty}\leq \frac{C}{n^r}$, for some $C<\infty$. Then, by continuity and sub-additivity of probability, the asymptotic Type I error is given by 
\begin{align}
\label{eq:type1}
&\lim_{n \to \infty} P_{\,\mathbf{H_0}}( \mathcal A_{n}) \leq \lim_{n \to \infty}  \sum_{-M_n \leq m \leq M_n} P_{\,\mathbf{H_0}}(S_{m,n} >t_{b,q_{n}}) = \notag\\  
&\lim_{n \to \infty} (2M_n+1) \left(1-F_n(t_{b,q_{n}})\right) \leq \lim_{n \to \infty} (2M_n+1) \left( 1 - (1-\frac{\alpha}{2M_n+1}) + \frac C {n^r}\right)= \alpha, 
\end{align}
as long as $M_n=o(n^r)$. Intuitively, we require that the number of tests being performed increases at a slower rate than the rate of distributional convergence of the bootstrapped statistics.%For the penultimate equation we have assumed that $S_{m,n}$ has cumulative distribution equal to $F$. This assumption could be easily removed should one apply Berry-Esseen like bound for degenerate $V$-statistics with $\tau$-mixing process. No such bound exist in the literature but its existence follows form the proof of the  \cite[Theorem 2.1]{leucht_dependent_2013}; the proof of such bound is however beyond the scope of this work.
 
On the other hand, under the alternative, there exists some $m$ for which $n^{-1} S_{m,n}$ converges to some positive constant. In this case however, we do not have a handle on the asymptotic distribution $F$ of $S_{m,n} = n V(h,Z^m)$: cumulative distribution function obtained from sampling $n V_{b2}(h)$ converges to $G$ (possibly different from $F$) with a finite variance, while the behaviour of $n V_{b1}(h)$ is unspecified. We can however show that for any such cumulative distribution function $G$, the Type II error still converges to zero since
\begin{align*}
&P_{\,\mathbf{H_1}}(\mathcal A_{n}) \geq P_{\,\mathbf{H_1}}( S_{m,n} > G^{-1}(q_{n}) ) = P_{\,\mathbf{H_1}}( n^{-1} S_{m,n} > n^{-1} G^{-1}(q_{n}) ) \to 1,
\end{align*}
which follows from Lemma  \ref{lem:FoverN} below that shows that $n^{-1} G^{-1}(q_{n})$ converges to zero. 

\begin{lemma}
\label{lem:FoverN}
If $X \sim G$ is a random variable such that $\ev X^2 <\infty$, $q_{n} = 1-\frac{\alpha}{2M_n+1}$ and $M_n=o(n)$ then $n^{-1} G^{-1}(q_{n}) \to 0$.
\end{lemma}  
\begin{proof}
 First observe that by Markov inequality $P(X \geq t) \leq \frac {\ev X^2} {t} $ and therefore $G(t) > g(t) = 1 - \frac {\ev X^2} {t}$.  Therefore, on the interval   $(\ev X,1)$,  $ \ G^{-1}(x) < g^{-1}(x) = \frac{\ev X^2}{1-x}$. As a result 
 \begin{align}
 \label{eg:aletrnative}
 n^{-1} G^{-1}(q_{n})  \leq  n^{-1} g^{-1}(q_{n})  =n^{-1}  \frac{\ev X^2} { 1 - (1 -\frac {\alpha} {2M_n+1})}= \frac{(2M_n+1) \ev X^2  }{\alpha n} \overset{n \to \infty}{\longrightarrow} 0.
 \end{align} 
\end{proof} 

\section{Various comments}

\subsection{Time complexity}
The original HSIC and MMD tests for i.i.d. data, the computational cost of the wild bootstrap approach scales quadratically in the number of samples, and linearly in the number of bootstrap iterations (in the i.i.d. case, these were permutations of the data). The main alternative approaches are the lagged bootstrap of \cite{chwialkowski2014kernel}, which has the same scaling with data and number of bootstraps, and the spectrogram approach of \cite{besserve_statistical_2013} (note, however, that both these alternative approaches apply only to the independence testing case). The cost of \cite{besserve_statistical_2013} is comparable to our approach, however the statistical power of \cite{besserve_statistical_2013} was much weaker on the data we examined.